{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import openai\n",
    "# Add the path to the constants file to the system path\n",
    "sys.path.append('../../')\n",
    "from constants import *\n",
    "from evaluation_utils import *\n",
    "from path_utils import *\n",
    "from ChatCompletion_OpenAI_API import *\n",
    "from CF_utils import *\n",
    "from MF_utils import *\n",
    "\n",
    "# OpenAI API Key\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# source code folder path\n",
    "rec_sys_dir = get_rec_sys_directory()\n",
    "print(f\"Rec-sys directory: {rec_sys_dir}\")\n",
    "\n",
    "# data folder path\n",
    "DATA_DIR = os.path.join(rec_sys_dir, 'data')\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "\n",
    "# data path\n",
    "data_path = os.path.join(DATA_DIR, 'amazon-beauty/large_merged_data.csv')\n",
    "print(f'Data path: {data_path}')\n",
    "\n",
    "# output\n",
    "\n",
    "CF_OUTPUT_PATH = os.path.join(DATA_DIR, 'amazon-beauty/output/large_CF_fewshot_output_path_ratings_per_user.csv')\n",
    "print(f'Data path: {CF_OUTPUT_PATH}')\n",
    "\n",
    "CF_RERUN_PATH = os.path.join(DATA_DIR, 'amazon-beauty/output/rerun_large_CF_fewshot_output_path_ratings_per_user.csv')\n",
    "print(f'Data path: {CF_RERUN_PATH}')\n",
    "\n",
    "\n",
    "# Constants for column names\n",
    "USER_COLUMN_NAME = 'reviewerID'\n",
    "TITLE_COLUMN_NAME = 'title'\n",
    "ITEM_ID_COLUMN = 'asin'\n",
    "RATING_COLUMN_NAME = 'rating'\n",
    "TIME_STAMP_COLUMN_NAME = 'unixReviewTime'\n",
    "\n",
    "# num_ratings_per_user\n",
    "NUM_RATINGS_PER_USER = 1\n",
    "# num_main_user_ratings\n",
    "NUM_MAIN_USER_RATINGS = 4\n",
    "# num_similar_users\n",
    "NUM_SIMILAR_USERS = 4\n",
    "\n",
    "SYSTEM_CONTENT = AMAZON_CONTENT_SYSTEM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (1.62.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/tnathu-ai/opt/anaconda3/envs/test/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2024 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# https://github.com/google-research/google-research/blob/master/dot_vs_learned_similarity/mf_simple.py\n",
    "\n",
    "r\"\"\"Evaluation of matrix factorization following the protocol of the NCF paper.\n",
    "\n",
    "Details:\n",
    " - Model: Matrix factorization with biases:\n",
    "     y(u,i) = b + v_{u,1}+v_{i,1}+\\sum_{f=2}^d v_{u,f}*v_{i,f}\n",
    " - Loss: logistic loss\n",
    " - Optimization algorithm: stochastic gradient descent\n",
    " - Negatives sampling: Random negatives are added during training\n",
    " - Optimization objective (similar to NCF paper)\n",
    "     argmin_V \\sum_{(u,i) \\in S} [\n",
    "          ln(1+exp(-y(u,i)))\n",
    "        + #neg/|I| * \\sum_{j \\in I} ln(1+exp(y(u,j)))\n",
    "        + reg * ||V||_2^2 ]\n",
    " - Evaluation follows the protocol from:\n",
    "   He, X., Liao, L., Zhang, H., Nie, L., Hu, X., and Chua, T.-S.: Neural\n",
    "   collaborative filtering. WWW 2017\n",
    "\"\"\"\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "import argparse\n",
    "# Dataset and evaluation protocols reused from\n",
    "# https://github.com/hexiangnan/neural_collaborative_filtering\n",
    "import numpy as np\n",
    "# NCF evaluation protocol\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from time import time\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "class MFModel(object):\n",
    "  \"\"\"A matrix factorization model trained using SGD and negative sampling.\"\"\"\n",
    "\n",
    "  def __init__(self, num_user, num_item, embedding_dim, reg, stddev):\n",
    "    \"\"\"Initializes MFModel.\n",
    "\n",
    "    Args:\n",
    "      num_user: the total number of users.\n",
    "      num_item: the total number of items.\n",
    "      embedding_dim: the embedding dimension.\n",
    "      reg: the regularization coefficient.\n",
    "      stddev: embeddings are initialized from a random distribution with this\n",
    "        standard deviation.\n",
    "    \"\"\"\n",
    "    self.user_embedding = np.random.normal(0, stddev, (num_user, embedding_dim))\n",
    "    self.item_embedding = np.random.normal(0, stddev, (num_item, embedding_dim))\n",
    "    self.user_bias = np.zeros([num_user])\n",
    "    self.item_bias = np.zeros([num_item])\n",
    "    self.bias = 0.0\n",
    "    self.reg = reg\n",
    "\n",
    "  def _predict_one(self, user, item):\n",
    "    \"\"\"Predicts the score of a user for an item.\"\"\"\n",
    "    return (self.bias + self.user_bias[user] + self.item_bias[item] +\n",
    "            np.dot(self.user_embedding[user], self.item_embedding[item]))\n",
    "\n",
    "  def predict(self, pairs, batch_size, verbose):\n",
    "    \"\"\"Computes predictions for a given set of user-item pairs.\n",
    "\n",
    "    Args:\n",
    "      pairs: A pair of lists (users, items) of the same length.\n",
    "      batch_size: unused.\n",
    "      verbose: unused.\n",
    "\n",
    "    Returns:\n",
    "      predictions: A list of the same length as users and items, such that\n",
    "      predictions[i] is the models prediction for (users[i], items[i]).\n",
    "    \"\"\"\n",
    "    del batch_size, verbose\n",
    "    num_examples = len(pairs[0])\n",
    "    assert num_examples == len(pairs[1])\n",
    "    predictions = np.empty(num_examples)\n",
    "    for i in range(num_examples):\n",
    "      predictions[i] = self._predict_one(pairs[0][i], pairs[1][i])\n",
    "    return predictions\n",
    "\n",
    "  def fit(self, positive_pairs, learning_rate, num_negatives):\n",
    "    \"\"\"Trains the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "      positive_pairs: an array of shape [n, 2], each row representing a positive\n",
    "        user-item pair.\n",
    "      learning_rate: the learning rate to use.\n",
    "      num_negatives: the number of negative items to sample for each positive.\n",
    "\n",
    "    Returns:\n",
    "      The logistic loss averaged across examples.\n",
    "    \"\"\"\n",
    "    # Convert to implicit format and sample negatives.\n",
    "    user_item_label_matrix = self._convert_ratings_to_implicit_data(\n",
    "        positive_pairs, num_negatives)\n",
    "    np.random.shuffle(user_item_label_matrix)\n",
    "\n",
    "    # Iterate over all examples and perform one SGD step.\n",
    "    num_examples = user_item_label_matrix.shape[0]\n",
    "    reg = self.reg\n",
    "    lr = learning_rate\n",
    "    sum_of_loss = 0.0\n",
    "    for i in range(num_examples):\n",
    "      (user, item, rating) = user_item_label_matrix[i, :]\n",
    "      user_emb = self.user_embedding[user]\n",
    "      item_emb = self.item_embedding[item]\n",
    "      prediction = self._predict_one(user, item)\n",
    "\n",
    "      if prediction > 0:\n",
    "        one_plus_exp_minus_pred = 1.0 + np.exp(-prediction)\n",
    "        sigmoid = 1.0 / one_plus_exp_minus_pred\n",
    "        this_loss = (np.log(one_plus_exp_minus_pred) +\n",
    "                     (1.0 - rating) * prediction)\n",
    "      else:\n",
    "        exp_pred = np.exp(prediction)\n",
    "        sigmoid = exp_pred / (1.0 + exp_pred)\n",
    "        this_loss = -rating * prediction + np.log(1.0 + exp_pred)\n",
    "\n",
    "      grad = rating - sigmoid\n",
    "\n",
    "      self.user_embedding[user, :] += lr * (grad * item_emb - reg * user_emb)\n",
    "      self.item_embedding[item, :] += lr * (grad * user_emb - reg * item_emb)\n",
    "      self.user_bias[user] += lr * (grad - reg * self.user_bias[user])\n",
    "      self.item_bias[item] += lr * (grad - reg * self.item_bias[item])\n",
    "      self.bias += lr * (grad - reg * self.bias)\n",
    "\n",
    "      sum_of_loss += this_loss\n",
    "\n",
    "    # Return the mean logistic loss.\n",
    "    return sum_of_loss / num_examples\n",
    "\n",
    "  def _convert_ratings_to_implicit_data(self, positive_pairs, num_negatives):\n",
    "    \"\"\"Converts a list of positive pairs into a two class dataset.\n",
    "\n",
    "    Args:\n",
    "      positive_pairs: an array of shape [n, 2], each row representing a positive\n",
    "        user-item pair.\n",
    "      num_negatives: the number of negative items to sample for each positive.\n",
    "    Returns:\n",
    "      An array of shape [n*(1 + num_negatives), 3], where each row is a tuple\n",
    "      (user, item, label). The examples are obtained as follows:\n",
    "      To each (user, item) pair in positive_pairs correspond:\n",
    "      * one positive example (user, item, 1)\n",
    "      * num_negatives negative examples (user, item', 0) where item' is sampled\n",
    "        uniformly at random.\n",
    "    \"\"\"\n",
    "    num_items = self.item_embedding.shape[0]\n",
    "    num_pos_examples = positive_pairs.shape[0]\n",
    "    training_matrix = np.empty([num_pos_examples * (1 + num_negatives), 3],\n",
    "                               dtype=np.int32)\n",
    "    index = 0\n",
    "    for pos_index in range(num_pos_examples):\n",
    "      u = positive_pairs[pos_index, 0]\n",
    "      i = positive_pairs[pos_index, 1]\n",
    "\n",
    "      # Treat the rating as a positive training instance\n",
    "      training_matrix[index] = [u, i, 1]\n",
    "      index += 1\n",
    "\n",
    "      # Add N negatives by sampling random items.\n",
    "      # This code does not enforce that the sampled negatives are not present in\n",
    "      # the training data. It is possible that the sampling procedure adds a\n",
    "      # negative that is already in the set of positives. It is also possible\n",
    "      # that an item is sampled twice. Both cases should be fine.\n",
    "      for _ in range(num_negatives):\n",
    "        j = np.random.randint(num_items)\n",
    "        training_matrix[index] = [u, j, 0]\n",
    "        index += 1\n",
    "    return training_matrix\n",
    "\n",
    "\n",
    "def evaluate(model, test_ratings, test_negatives, K=10):\n",
    "  \"\"\"Helper that calls evaluate from the NCF libraries.\"\"\"\n",
    "  (hits, ndcgs) = evaluate_model(model, test_ratings, test_negatives, K=K,\n",
    "                                 num_thread=1)\n",
    "  return np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "\n",
    "\n",
    "'''\n",
    "Created on Aug 9, 2016\n",
    "Keras Implementation of Neural Matrix Factorization (NeuMF) recommender model in:\n",
    "He Xiangnan et al. Neural Collaborative Filtering. In WWW 2017.  \n",
    "\n",
    "@author: Xiangnan He (xiangnanhe@gmail.com)\n",
    "'''\n",
    "def get_NCF_model(num_users, num_items, mf_dim=10, layers=[10], reg_layers=[0], reg_mf=0):\n",
    "    assert len(layers) == len(reg_layers)\n",
    "    num_layer = len(layers) #Number of layers in the MLP\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "    \n",
    "    # Embedding layer\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = mf_dim, name = 'mf_embedding_user',\n",
    "                                  init = init_normal, W_regularizer = l2(reg_mf), input_length=1)\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = mf_dim, name = 'mf_embedding_item',\n",
    "                                  init = init_normal, W_regularizer = l2(reg_mf), input_length=1)   \n",
    "\n",
    "    MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = layers[0]/2, name = \"mlp_embedding_user\",\n",
    "                                  init = init_normal, W_regularizer = l2(reg_layers[0]), input_length=1)\n",
    "    MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = layers[0]/2, name = 'mlp_embedding_item',\n",
    "                                  init = init_normal, W_regularizer = l2(reg_layers[0]), input_length=1)   \n",
    "    \n",
    "    # MF part\n",
    "    mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    mf_vector = merge([mf_user_latent, mf_item_latent], mode = 'mul') # element-wise multiply\n",
    "\n",
    "    # MLP part \n",
    "    mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "    mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
    "    mlp_vector = merge([mlp_user_latent, mlp_item_latent], mode = 'concat')\n",
    "    for idx in xrange(1, num_layer):\n",
    "        layer = Dense(layers[idx], W_regularizer= l2(reg_layers[idx]), activation='relu', name=\"layer%d\" %idx)\n",
    "        mlp_vector = layer(mlp_vector)\n",
    "\n",
    "    # Concatenate MF and MLP parts\n",
    "    #mf_vector = Lambda(lambda x: x * alpha)(mf_vector)\n",
    "    #mlp_vector = Lambda(lambda x : x * (1-alpha))(mlp_vector)\n",
    "    predict_vector = merge([mf_vector, mlp_vector], mode = 'concat')\n",
    "    \n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation='sigmoid', init='lecun_uniform', name = \"prediction\")(predict_vector)\n",
    "    \n",
    "    model = Model(input=[user_input, item_input], \n",
    "                  output=prediction)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Similarity Scores for Users\n",
    "def calculate_MF_similarity_user(user_factors):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between users based on their latent factors from MF.\n",
    "    \n",
    "    Args:\n",
    "        user_factors (numpy.ndarray): The matrix of user latent factors.\n",
    "    \n",
    "    Returns:\n",
    "        user_similarity_matrix (numpy.ndarray): A matrix of user-user similarity scores.\n",
    "    \"\"\"\n",
    "    # Normalize user factors to unit vectors\n",
    "    norms = np.linalg.norm(user_factors, axis=1, keepdims=True)\n",
    "    normalized_user_factors = user_factors / norms\n",
    "    # Calculate cosine similarity\n",
    "    user_similarity_matrix = np.dot(normalized_user_factors, normalized_user_factors.T)\n",
    "    return user_similarity_matrix\n",
    "\n",
    "\n",
    "# Similarity Scores for Items\n",
    "def calculate_MF_similarity_item(item_factors):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between items based on their latent factors from MF.\n",
    "    \n",
    "    Args:\n",
    "        item_factors (numpy.ndarray): The matrix of item latent factors.\n",
    "    \n",
    "    Returns:\n",
    "        item_similarity_matrix (numpy.ndarray): A matrix of item-item similarity scores.\n",
    "    \"\"\"\n",
    "    # Normalize item factors to unit vectors\n",
    "    norms = np.linalg.norm(item_factors, axis=1, keepdims=True)\n",
    "    normalized_item_factors = item_factors / norms\n",
    "    # Calculate cosine similarity\n",
    "    item_similarity_matrix = np.dot(normalized_item_factors, normalized_item_factors.T)\n",
    "    return item_similarity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Input, Flatten, Multiply, Concatenate, Dense\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Input, Flatten, Multiply, Concatenate, Dense\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def get_NCF_model(num_users, num_items, mf_dim=10, layers=[10], reg_layers=[0], reg_mf=0):\n",
    "    assert len(layers) == len(reg_layers)\n",
    "    num_layer = len(layers)  # Number of layers in the MLP\n",
    "\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name='item_input')\n",
    "    \n",
    "    # Embedding layer\n",
    "    MF_Embedding_User = Embedding(input_dim=num_users, output_dim=mf_dim, name='mf_embedding_user',\n",
    "                                  embeddings_initializer=RandomNormal(), embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "    MF_Embedding_Item = Embedding(input_dim=num_items, output_dim=mf_dim, name='mf_embedding_item',\n",
    "                                  embeddings_initializer=RandomNormal(), embeddings_regularizer=l2(reg_mf), input_length=1)\n",
    "\n",
    "    MLP_Embedding_User = Embedding(input_dim=num_users, output_dim=int(layers[0] / 2), name=\"mlp_embedding_user\",\n",
    "                                   embeddings_initializer=RandomNormal(), embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
    "    MLP_Embedding_Item = Embedding(input_dim=num_items, output_dim=int(layers[0] / 2), name='mlp_embedding_item',\n",
    "                                   embeddings_initializer=RandomNormal(), embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
    "    \n",
    "    # MF part\n",
    "    mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    mf_vector = Multiply()([mf_user_latent, mf_item_latent])\n",
    "\n",
    "    # MLP part \n",
    "    mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "    mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
    "    mlp_vector = Concatenate()([mlp_user_latent, mlp_item_latent])\n",
    "    \n",
    "    for idx in range(1, num_layer):\n",
    "        layer = Dense(layers[idx], activation='relu', name=\"layer%d\" % idx, kernel_regularizer=l2(reg_layers[idx]))\n",
    "        mlp_vector = layer(mlp_vector)\n",
    "\n",
    "    # Concatenate MF and MLP parts\n",
    "    predict_vector = Concatenate()([mf_vector, mlp_vector])\n",
    "    \n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name=\"prediction\")(predict_vector)\n",
    "    \n",
    "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleMatrixFactorization:\n",
    "    def __init__(self, num_users, num_items, num_features=10, learning_rate=0.01, reg=0.1, epochs=10):\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg = reg\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        # Initialize user and item latent feature matrices\n",
    "        self.user_features = np.random.normal(scale=1./num_features, size=(num_users, num_features))\n",
    "        self.item_features = np.random.normal(scale=1./num_features, size=(num_items, num_features))\n",
    "\n",
    "        # Initialize the biases\n",
    "        self.user_bias = np.zeros(num_users)\n",
    "        self.item_bias = np.zeros(num_items)\n",
    "        self.global_bias = np.mean(self.user_features.dot(self.item_features.T))\n",
    "\n",
    "    def train(self, train_data):\n",
    "        \"\"\"\n",
    "        Train the matrix factorization model.\n",
    "        \n",
    "        Args:\n",
    "        train_data (list of tuples): List of (user, item, rating) tuples.\n",
    "        \"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            np.random.shuffle(train_data)\n",
    "            for user, item, rating in train_data:\n",
    "                prediction = self.predict(user, item)\n",
    "                error = rating - prediction\n",
    "                \n",
    "                # Update biases\n",
    "                self.user_bias[user] += self.learning_rate * (error - self.reg * self.user_bias[user])\n",
    "                self.item_bias[item] += self.learning_rate * (error - self.reg * self.item_bias[item])\n",
    "                \n",
    "                # Update latent features\n",
    "                self.user_features[user, :] += self.learning_rate * (error * self.item_features[item, :] - self.reg * self.user_features[user, :])\n",
    "                self.item_features[item, :] += self.learning_rate * (error * self.user_features[user, :] - self.reg * self.item_features[item, :])\n",
    "                \n",
    "    def predict(self, user, item):\n",
    "        \"\"\"\n",
    "        Predict a rating by the user for the item.\n",
    "        \"\"\"\n",
    "        prediction = self.global_bias + self.user_bias[user] + self.item_bias[item] + self.user_features[user, :].dot(self.item_features[item, :].T)\n",
    "        return prediction\n",
    "\n",
    "# Example usage:\n",
    "# Assume we have 5 users, 10 items, and we're working with 3 features for this example.\n",
    "mf = SimpleMatrixFactorization(num_users=5, num_items=10, num_features=3)\n",
    "\n",
    "# Example training data: (user, item, rating)\n",
    "train_data = [(0, 0, 5), (1, 2, 3), (2, 1, 4), (3, 0, 1), (4, 1, 5)]\n",
    "mf.train(train_data)\n",
    "\n",
    "# Predicting the rating of user 0 for item 1\n",
    "print(mf.predict(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "1.2801839971143159\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleMatrixFactorization:\n",
    "    def __init__(self, num_users, num_items, num_features=10, learning_rate=0.01, reg=0.1, epochs=10):\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_features = num_features\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg = reg\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        # Initialize user and item latent feature matrices\n",
    "        self.user_features = np.random.normal(scale=1./num_features, size=(num_users, num_features))\n",
    "        self.item_features = np.random.normal(scale=1./num_features, size=(num_items, num_features))\n",
    "\n",
    "        # Initialize the biases\n",
    "        self.user_bias = np.zeros(num_users)\n",
    "        self.item_bias = np.zeros(num_items)\n",
    "        self.global_bias = np.mean(self.user_features.dot(self.item_features.T))\n",
    "\n",
    "    def train(self, train_data):\n",
    "        \"\"\"\n",
    "        Train the matrix factorization model.\n",
    "        \n",
    "        Args:\n",
    "        train_data (list of tuples): List of (user, item, rating) tuples.\n",
    "        \"\"\"\n",
    "        for epoch in range(self.epochs):\n",
    "            np.random.shuffle(train_data)\n",
    "            for user, item, rating in train_data:\n",
    "                prediction = self.predict(user, item)\n",
    "                error = rating - prediction\n",
    "                \n",
    "                # Update biases\n",
    "                self.user_bias[user] += self.learning_rate * (error - self.reg * self.user_bias[user])\n",
    "                self.item_bias[item] += self.learning_rate * (error - self.reg * self.item_bias[item])\n",
    "                \n",
    "                # Update latent features\n",
    "                self.user_features[user, :] += self.learning_rate * (error * self.item_features[item, :] - self.reg * self.user_features[user, :])\n",
    "                self.item_features[item, :] += self.learning_rate * (error * self.user_features[user, :] - self.reg * self.item_features[item, :])\n",
    "                \n",
    "    def predict(self, user, item):\n",
    "        \"\"\"\n",
    "        Predict a rating by the user for the item.\n",
    "        \"\"\"\n",
    "        prediction = self.global_bias + self.user_bias[user] + self.item_bias[item] + self.user_features[user, :].dot(self.item_features[item, :].T)\n",
    "        return prediction\n",
    "\n",
    "# Example usage:\n",
    "# Assume we have 5 users, 10 items, and we're working with 3 features for this example.\n",
    "mf = SimpleMatrixFactorization(num_users=5, num_items=10, num_features=3)\n",
    "\n",
    "# Example training data: (user, item, rating)\n",
    "train_data = [(0, 0, 5), (1, 2, 3), (2, 1, 4), (3, 0, 1), (4, 1, 5)]\n",
    "mf.train(train_data)\n",
    "\n",
    "# Predicting the rating of user 0 for item 1\n",
    "print(mf.predict(0, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m CF_OUTPUT_TIMESTAMP_PATH \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamazon-beauty/output/split_timestamp/timestamp_large_CF_fewshot_output_path_ratings_per_user.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCF_OUTPUT_TIMESTAMP_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m CF_RERUN_TIMESTAMP_PATH \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamazon-beauty/output/split_timestamp/rerun_timestamp_large_CF_fewshot_output_path_ratings_per_user.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "CF_OUTPUT_TIMESTAMP_PATH = os.path.join(DATA_DIR, 'amazon-beauty/output/split_timestamp/timestamp_large_CF_fewshot_output_path_ratings_per_user.csv')\n",
    "print(f'Data path: {CF_OUTPUT_TIMESTAMP_PATH}')\n",
    "\n",
    "CF_RERUN_TIMESTAMP_PATH = os.path.join(DATA_DIR, 'amazon-beauty/output/split_timestamp/rerun_timestamp_large_CF_fewshot_output_path_ratings_per_user.csv')\n",
    "print(f'Data path: {CF_RERUN_TIMESTAMP_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = data[USER_COLUMN_NAME].nunique()\n",
    "num_items = data[ITEM_ID_COLUMN].nunique()\n",
    "\n",
    "print(f\"Number of users: {num_users}\")\n",
    "print(f\"Number of items: {num_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create User-Item Interaction Matrix\n",
    "interaction_matrix = pd.pivot_table(data, index=USER_COLUMN_NAME, columns=ITEM_ID_COLUMN, values=RATING_COLUMN_NAME).fillna(0)\n",
    "csr_interaction_matrix = csr_matrix(interaction_matrix.values)\n",
    "\n",
    "interaction_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_interaction_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Compute the user-user Pearson Correlation Coefficient Matrix\n",
    "user_pcc_matrix = pearson_correlation(csr_interaction_matrix)\n",
    "print(f'User PCC Matrix:\\n{user_pcc_matrix}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the item-item Pearson Correlation Coefficient Matrix\n",
    "# Assuming the function 'item_pearson_correlation' takes a dense matrix as input.\n",
    "# If it still takes a csr_matrix, then convert it inside the function.\n",
    "dense_interaction_matrix = csr_interaction_matrix.toarray()\n",
    "\n",
    "item_pcc_matrix = item_pearson_correlation(dense_interaction_matrix.T)\n",
    "print(f'Item PCC Matrix:\\n{item_pcc_matrix}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results_df = predict_ratings_with_CF_item_and_save(\n",
    "    data=data, \n",
    "    user_pcc_matrix=user_pcc_matrix, \n",
    "    item_pcc_matrix=item_pcc_matrix,\n",
    "    user_column_name=USER_COLUMN_NAME, \n",
    "    movie_column_name=TITLE_COLUMN_NAME, \n",
    "    movie_id_column=ITEM_ID_COLUMN,\n",
    "    rating_column_name=RATING_COLUMN_NAME, \n",
    "    num_ratings_per_user=NUM_RATINGS_PER_USER, \n",
    "    num_similar_users=NUM_SIMILAR_USERS,\n",
    "    num_main_user_ratings=NUM_MAIN_USER_RATINGS,\n",
    "    test_selection_method='random',\n",
    "    save_path=CF_OUTPUT_PATH, \n",
    "    seed=RANDOM_STATE\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate updated CF model predictions\n",
    "evaluate_model_predictions_rmse_mae(\n",
    "    data_path=CF_OUTPUT_PATH,\n",
    "    num_examples=NUM_EXAMPLES,\n",
    "    actual_ratings_column='actual_rating',\n",
    "    predicted_ratings_column='predicted_rating'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split by Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results_df = predict_ratings_with_CF_item_and_save(\n",
    "    data=data, \n",
    "    user_pcc_matrix=user_pcc_matrix, \n",
    "    item_pcc_matrix=item_pcc_matrix,\n",
    "    user_column_name=USER_COLUMN_NAME, \n",
    "    movie_column_name=TITLE_COLUMN_NAME, \n",
    "    timestamp_column_name=TIME_STAMP_COLUMN_NAME,\n",
    "    movie_id_column=ITEM_ID_COLUMN,\n",
    "    rating_column_name=RATING_COLUMN_NAME, \n",
    "    num_ratings_per_user=NUM_RATINGS_PER_USER, \n",
    "    num_similar_users=NUM_SIMILAR_USERS,\n",
    "    num_main_user_ratings=NUM_MAIN_USER_RATINGS,\n",
    "    test_selection_method='sequential',\n",
    "    save_path=CF_OUTPUT_TIMESTAMP_PATH, \n",
    "    seed=RANDOM_STATE\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_data = pd.read_csv(CF_OUTPUT_TIMESTAMP_PATH)\n",
    "\n",
    "# Display the original data types\n",
    "print(\"Original Data Types:\")\n",
    "print(saved_data.dtypes)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Attempt to convert ratings to float and add a flag for conversion failure\n",
    "saved_data['is_rating_float'] = pd.to_numeric(saved_data['predicted_rating'], errors='coerce').notna()\n",
    "\n",
    "# Filter rows where ratings are not float\n",
    "non_float_ratings = saved_data[saved_data['is_rating_float'] == False]\n",
    "\n",
    "# total number of rows with non-float ratings\n",
    "print(f\"Total number of rows with non-float ratings: {len(non_float_ratings)}\")\n",
    "\n",
    "# rerun indices for non-float ratings\n",
    "rerun_indices = non_float_ratings.index.tolist()\n",
    "print(f\"Rerun indices: {rerun_indices}\")\n",
    "\n",
    "# Display rows with non-float ratings\n",
    "print(\"Rows with non-float ratings:\")\n",
    "non_float_ratings.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rerun_failed_CF_item_PCC_predictions_sequential(data, user_pcc_matrix, item_pcc_matrix,\n",
    "                                         save_path, user_column_name, movie_column_name,\n",
    "                                         movie_id_column, rating_column_name,\n",
    "                                         num_ratings_per_user, num_main_user_ratings, num_similar_users,\n",
    "                                         new_path, rerun_indices, seed=RANDOM_STATE,\n",
    "                                         system_content=AMAZON_CONTENT_SYSTEM):\n",
    "    # Load the original predictions\n",
    "    original_data = pd.read_csv(save_path)\n",
    "    original_data.columns = ['user_id', 'item_id', 'title', 'actual_rating', 'predicted_rating']\n",
    "\n",
    "    # Re-seed for reproducibility\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Map unique users and items to their indices for quick access\n",
    "    unique_users = data[user_column_name].unique()\n",
    "    unique_items = data[movie_id_column].unique()\n",
    "    user_id_to_index = {user_id: idx for idx, user_id in enumerate(unique_users)}\n",
    "    item_id_to_index = {item_id: idx for idx, item_id in enumerate(unique_items)}\n",
    "\n",
    "    for index in rerun_indices:\n",
    "        user_id = original_data.at[index, 'user_id']\n",
    "        item_id = original_data.at[index, 'item_id']\n",
    "        user_idx = user_id_to_index.get(user_id)\n",
    "        item_idx = item_id_to_index.get(item_id)\n",
    "\n",
    "        if user_idx is None or item_idx is None:\n",
    "            print(f\"User ID: {user_id} or Item ID: {item_id} not found in index. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Rerunning prediction for User ID: {user_id}, Item ID: {item_id} (Index: {index})\")\n",
    "\n",
    "        # Retrieve user's and item's data\n",
    "        user_data = data[data[user_column_name] == user_id]\n",
    "        item_data = data[data[movie_id_column] == item_id]\n",
    "\n",
    "        if item_data.empty:\n",
    "            print(f\"Item data for ID: {item_id} not found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Sample user's historical ratings\n",
    "        if len(user_data) < num_main_user_ratings:\n",
    "            main_user_ratings = user_data\n",
    "        else:\n",
    "            main_user_ratings = user_data.sample(n=num_main_user_ratings, random_state=seed)\n",
    "\n",
    "        # Construct the context from the user's ratings\n",
    "        main_user_ratings_str = '\\n'.join([\n",
    "            f\"* Title: {row[movie_column_name]}, Rating: {row[rating_column_name]} stars\"\n",
    "            for _, row in main_user_ratings.iterrows()\n",
    "        ])\n",
    "\n",
    "        # Identify similar users and items\n",
    "        similar_users_idx = np.argsort(-user_pcc_matrix[user_idx])[:num_similar_users + 1]\n",
    "        similar_users_idx = similar_users_idx[similar_users_idx != user_idx][:num_similar_users]\n",
    "\n",
    "        similar_items_idx = np.argsort(-item_pcc_matrix[item_idx])[:num_similar_users + 1]\n",
    "        similar_items_idx = similar_items_idx[similar_items_idx != item_idx][:num_similar_users]\n",
    "\n",
    "        # Compile ratings from similar users and items\n",
    "        similar_users_ratings = \"\"\n",
    "        for idx in similar_users_idx:\n",
    "            similar_user_id = unique_users[idx]\n",
    "            similar_user_data = data[data[user_column_name] == similar_user_id]\n",
    "            for _, row in similar_user_data.iterrows():\n",
    "                similar_users_ratings += f\"* Title: {row[movie_column_name]}, Rating: {row[rating_column_name]} stars\\n\"\n",
    "\n",
    "        # Predict the rating\n",
    "        combined_text = f\"Title: {item_data.iloc[0][movie_column_name]}\"\n",
    "        prompt = f\"Main User Ratings:\\n{main_user_ratings_str}\\n\\nSimilar Users' Ratings:\\n{similar_users_ratings}\\n\\nPredict rating for '{combined_text}':\"\n",
    "        predicted_rating = predict_rating_combined_ChatCompletion(\n",
    "            combined_text, approach=\"CF\", similar_users_ratings=similar_users_ratings,\n",
    "            rating_history=main_user_ratings_str, system_content=system_content\n",
    "        )\n",
    "\n",
    "        # Update the original data with the new prediction\n",
    "        original_data.at[index, 'predicted_rating'] = predicted_rating\n",
    "        print(f\"Updated prediction for User ID: {user_id}, Item ID: {item_id}: {predicted_rating}\")\n",
    "\n",
    "    # Save the updated predictions to a new file\n",
    "    original_data.to_csv(new_path, index=False)\n",
    "    print(f\"Updated predictions saved to {new_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rerun_failed_CF_item_PCC_predictions_sequential(data, \n",
    "                                    user_pcc_matrix=user_pcc_matrix,  \n",
    "                                    item_pcc_matrix=item_pcc_matrix,\n",
    "                                    save_path=CF_OUTPUT_TIMESTAMP_PATH,\n",
    "                                    user_column_name=USER_COLUMN_NAME,\n",
    "                                    movie_column_name=TITLE_COLUMN_NAME,\n",
    "                                    movie_id_column=ITEM_ID_COLUMN,\n",
    "                                    rating_column_name=RATING_COLUMN_NAME, \n",
    "                                    num_ratings_per_user=NUM_RATINGS_PER_USER, \n",
    "                                    num_similar_users=NUM_SIMILAR_USERS,\n",
    "                                    num_main_user_ratings=NUM_MAIN_USER_RATINGS,\n",
    "                                    new_path=CF_RERUN_TIMESTAMP_PATH,\n",
    "                                    rerun_indices=rerun_indices\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate updated CF model predictions\n",
    "evaluate_model_predictions_rmse_mae(\n",
    "    data_path=CF_RERUN_TIMESTAMP_PATH,\n",
    "    num_examples=NUM_EXAMPLES,\n",
    "    actual_ratings_column='actual_rating',\n",
    "    predicted_ratings_column='predicted_rating'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CF using using Matrix Factorization\n",
    "\n",
    "\n",
    "+ Preparing the user-item interaction matrix.\n",
    "+ Performing matrix factorization to obtain latent factors for users and items.\n",
    "+ Calculating similarities between users or items using the latent factors.\n",
    "+ Selecting similar users or items based on these similarities.\n",
    "+ Using the information from similar users or items to predict ratings for a given user-item pair.\n",
    "+ Feeding these predictions into the OpenAI ChatCompletion API as part of a collaborative filtering approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2024 The Google Research Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "r\"\"\"Evaluation of matrix factorization following the protocol of the NCF paper.\n",
    "\n",
    "Details:\n",
    " - Model: Matrix factorization with biases:\n",
    "     y(u,i) = b + v_{u,1}+v_{i,1}+\\sum_{f=2}^d v_{u,f}*v_{i,f}\n",
    " - Loss: logistic loss\n",
    " - Optimization algorithm: stochastic gradient descent\n",
    " - Negatives sampling: Random negatives are added during training\n",
    " - Optimization objective (similar to NCF paper)\n",
    "     argmin_V \\sum_{(u,i) \\in S} [\n",
    "          ln(1+exp(-y(u,i)))\n",
    "        + #neg/|I| * \\sum_{j \\in I} ln(1+exp(y(u,j)))\n",
    "        + reg * ||V||_2^2 ]\n",
    " - Evaluation follows the protocol from:\n",
    "   He, X., Liao, L., Zhang, H., Nie, L., Hu, X., and Chua, T.-S.: Neural\n",
    "   collaborative filtering. WWW 2017\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "# Dataset and evaluation protocols reused from\n",
    "# https://github.com/hexiangnan/neural_collaborative_filtering\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MFModel(object):\n",
    "  \"\"\"A matrix factorization model trained using SGD and negative sampling.\"\"\"\n",
    "\n",
    "  def __init__(self, num_user, num_item, embedding_dim, reg, stddev):\n",
    "    \"\"\"Initializes MFModel.\n",
    "\n",
    "    Args:\n",
    "      num_user: the total number of users.\n",
    "      num_item: the total number of items.\n",
    "      embedding_dim: the embedding dimension.\n",
    "      reg: the regularization coefficient.\n",
    "      stddev: embeddings are initialized from a random distribution with this\n",
    "        standard deviation.\n",
    "    \"\"\"\n",
    "    self.user_embedding = np.random.normal(0, stddev, (num_user, embedding_dim))\n",
    "    self.item_embedding = np.random.normal(0, stddev, (num_item, embedding_dim))\n",
    "    self.user_bias = np.zeros([num_user])\n",
    "    self.item_bias = np.zeros([num_item])\n",
    "    self.bias = 0.0\n",
    "    self.reg = reg\n",
    "\n",
    "  def _predict_one(self, user, item):\n",
    "    \"\"\"Predicts the score of a user for an item.\"\"\"\n",
    "    return (self.bias + self.user_bias[user] + self.item_bias[item] +\n",
    "            np.dot(self.user_embedding[user], self.item_embedding[item]))\n",
    "\n",
    "  def predict(self, pairs, batch_size, verbose):\n",
    "    \"\"\"Computes predictions for a given set of user-item pairs.\n",
    "\n",
    "    Args:\n",
    "      pairs: A pair of lists (users, items) of the same length.\n",
    "      batch_size: unused.\n",
    "      verbose: unused.\n",
    "\n",
    "    Returns:\n",
    "      predictions: A list of the same length as users and items, such that\n",
    "      predictions[i] is the models prediction for (users[i], items[i]).\n",
    "    \"\"\"\n",
    "    del batch_size, verbose\n",
    "    num_examples = len(pairs[0])\n",
    "    assert num_examples == len(pairs[1])\n",
    "    predictions = np.empty(num_examples)\n",
    "    for i in range(num_examples):\n",
    "      predictions[i] = self._predict_one(pairs[0][i], pairs[1][i])\n",
    "    return predictions\n",
    "\n",
    "  def fit(self, positive_pairs, learning_rate, num_negatives):\n",
    "    \"\"\"Trains the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "      positive_pairs: an array of shape [n, 2], each row representing a positive\n",
    "        user-item pair.\n",
    "      learning_rate: the learning rate to use.\n",
    "      num_negatives: the number of negative items to sample for each positive.\n",
    "\n",
    "    Returns:\n",
    "      The logistic loss averaged across examples.\n",
    "    \"\"\"\n",
    "    # Convert to implicit format and sample negatives.\n",
    "    user_item_label_matrix = self._convert_ratings_to_implicit_data(\n",
    "        positive_pairs, num_negatives)\n",
    "    np.random.shuffle(user_item_label_matrix)\n",
    "\n",
    "    # Iterate over all examples and perform one SGD step.\n",
    "    num_examples = user_item_label_matrix.shape[0]\n",
    "    reg = self.reg\n",
    "    lr = learning_rate\n",
    "    sum_of_loss = 0.0\n",
    "    for i in range(num_examples):\n",
    "      (user, item, rating) = user_item_label_matrix[i, :]\n",
    "      user_emb = self.user_embedding[user]\n",
    "      item_emb = self.item_embedding[item]\n",
    "      prediction = self._predict_one(user, item)\n",
    "\n",
    "      if prediction > 0:\n",
    "        one_plus_exp_minus_pred = 1.0 + np.exp(-prediction)\n",
    "        sigmoid = 1.0 / one_plus_exp_minus_pred\n",
    "        this_loss = (np.log(one_plus_exp_minus_pred) +\n",
    "                     (1.0 - rating) * prediction)\n",
    "      else:\n",
    "        exp_pred = np.exp(prediction)\n",
    "        sigmoid = exp_pred / (1.0 + exp_pred)\n",
    "        this_loss = -rating * prediction + np.log(1.0 + exp_pred)\n",
    "\n",
    "      grad = rating - sigmoid\n",
    "\n",
    "      self.user_embedding[user, :] += lr * (grad * item_emb - reg * user_emb)\n",
    "      self.item_embedding[item, :] += lr * (grad * user_emb - reg * item_emb)\n",
    "      self.user_bias[user] += lr * (grad - reg * self.user_bias[user])\n",
    "      self.item_bias[item] += lr * (grad - reg * self.item_bias[item])\n",
    "      self.bias += lr * (grad - reg * self.bias)\n",
    "\n",
    "      sum_of_loss += this_loss\n",
    "\n",
    "    # Return the mean logistic loss.\n",
    "    return sum_of_loss / num_examples\n",
    "\n",
    "  def _convert_ratings_to_implicit_data(self, positive_pairs, num_negatives):\n",
    "    \"\"\"Converts a list of positive pairs into a two class dataset.\n",
    "\n",
    "    Args:\n",
    "      positive_pairs: an array of shape [n, 2], each row representing a positive\n",
    "        user-item pair.\n",
    "      num_negatives: the number of negative items to sample for each positive.\n",
    "    Returns:\n",
    "      An array of shape [n*(1 + num_negatives), 3], where each row is a tuple\n",
    "      (user, item, label). The examples are obtained as follows:\n",
    "      To each (user, item) pair in positive_pairs correspond:\n",
    "      * one positive example (user, item, 1)\n",
    "      * num_negatives negative examples (user, item', 0) where item' is sampled\n",
    "        uniformly at random.\n",
    "    \"\"\"\n",
    "    num_items = self.item_embedding.shape[0]\n",
    "    num_pos_examples = positive_pairs.shape[0]\n",
    "    training_matrix = np.empty([num_pos_examples * (1 + num_negatives), 3],\n",
    "                               dtype=np.int32)\n",
    "    index = 0\n",
    "    for pos_index in range(num_pos_examples):\n",
    "      u = positive_pairs[pos_index, 0]\n",
    "      i = positive_pairs[pos_index, 1]\n",
    "\n",
    "      # Treat the rating as a positive training instance\n",
    "      training_matrix[index] = [u, i, 1]\n",
    "      index += 1\n",
    "\n",
    "      # Add N negatives by sampling random items.\n",
    "      # This code does not enforce that the sampled negatives are not present in\n",
    "      # the training data. It is possible that the sampling procedure adds a\n",
    "      # negative that is already in the set of positives. It is also possible\n",
    "      # that an item is sampled twice. Both cases should be fine.\n",
    "      for _ in range(num_negatives):\n",
    "        j = np.random.randint(num_items)\n",
    "        training_matrix[index] = [u, j, 0]\n",
    "        index += 1\n",
    "    return training_matrix\n",
    "\n",
    "\n",
    "def evaluate(model, test_ratings, test_negatives, K=10):\n",
    "  \"\"\"Helper that calls evaluate from the NCF libraries.\"\"\"\n",
    "  (hits, ndcgs) = evaluate_model(model, test_ratings, test_negatives, K=K,\n",
    "                                 num_thread=1)\n",
    "  return np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "embedding_dim = 20  # or any other number of your choice\n",
    "reg = 0.0  # Regularization coefficient\n",
    "stddev = 0.1  # Standard deviation for initialization\n",
    "\n",
    "model = MFModel(num_users, num_items, embedding_dim, reg, stddev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train the model\n",
    "\n",
    "# Create mappings for user and item IDs\n",
    "user_ids = data[USER_COLUMN_NAME].unique()\n",
    "user_id_mapping = {id: index for index, id in enumerate(user_ids)}\n",
    "item_ids = data[ITEM_ID_COLUMN].unique()\n",
    "item_id_mapping = {id: index for index, id in enumerate(item_ids)}\n",
    "\n",
    "# Apply mappings to the data\n",
    "data['mapped_user_id'] = data[USER_COLUMN_NAME].map(user_id_mapping)\n",
    "data['mapped_item_id'] = data[ITEM_ID_COLUMN].map(item_id_mapping)\n",
    "\n",
    "# Prepare positive_pairs with mapped IDs\n",
    "positive_pairs = data[['mapped_user_id', 'mapped_item_id']].values\n",
    "\n",
    "# proceed with training\n",
    "model.fit(positive_pairs, learning_rate, num_negatives)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity Scores for Users\n",
    "def calculate_MF_similarity_user(user_factors):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between users based on their latent factors from MF.\n",
    "    \n",
    "    Args:\n",
    "        user_factors (numpy.ndarray): The matrix of user latent factors.\n",
    "    \n",
    "    Returns:\n",
    "        user_similarity_matrix (numpy.ndarray): A matrix of user-user similarity scores.\n",
    "    \"\"\"\n",
    "    # Normalize user factors to unit vectors\n",
    "    norms = np.linalg.norm(user_factors, axis=1, keepdims=True)\n",
    "    normalized_user_factors = user_factors / norms\n",
    "    # Calculate cosine similarity\n",
    "    user_similarity_matrix = np.dot(normalized_user_factors, normalized_user_factors.T)\n",
    "    return user_similarity_matrix\n",
    "\n",
    "\n",
    "# Similarity Scores for Users\n",
    "def calculate_MF_similarity_item(item_factors):\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between items based on their latent factors from MF.\n",
    "    \n",
    "    Args:\n",
    "        item_factors (numpy.ndarray): The matrix of item latent factors.\n",
    "    \n",
    "    Returns:\n",
    "        item_similarity_matrix (numpy.ndarray): A matrix of item-item similarity scores.\n",
    "    \"\"\"\n",
    "    # Normalize item factors to unit vectors\n",
    "    norms = np.linalg.norm(item_factors, axis=1, keepdims=True)\n",
    "    normalized_item_factors = item_factors / norms\n",
    "    # Calculate cosine similarity\n",
    "    item_similarity_matrix = np.dot(normalized_item_factors, normalized_item_factors.T)\n",
    "    return item_similarity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_factors = model.user_embedding\n",
    "item_factors = model.item_embedding\n",
    "\n",
    "user_similarity_matrix = calculate_MF_similarity_user(user_factors)\n",
    "item_similarity_matrix = calculate_MF_similarity_item(item_factors)\n",
    "\n",
    "# Display shapes to verify outputs\n",
    "(user_factors.shape, item_factors.shape, user_similarity_matrix.shape, item_similarity_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CF_OUTPUT_TIMESTAMP_PATH = os.path.join(DATA_DIR, 'amazon-beauty/output/split_timestamp/sequential_MF_CF_fewshot_.csv')\n",
    "print(f'Data path: {CF_OUTPUT_TIMESTAMP_PATH}')\n",
    "\n",
    "CF_RERUN_TIMESTAMP_PATH = os.path.join(DATA_DIR, 'amazon-beauty/output/split_timestamp/rerun_sequential_MF_CF_fewshot.csv')\n",
    "print(f'Data path: {CF_RERUN_TIMESTAMP_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results_df = predict_ratings_with_CF_item_and_save(\n",
    "    data=data, \n",
    "    user_pcc_matrix=user_similarity_matrix, \n",
    "    item_pcc_matrix=item_similarity_matrix,\n",
    "    user_column_name=USER_COLUMN_NAME, \n",
    "    movie_column_name=TITLE_COLUMN_NAME, \n",
    "    timestamp_column_name=TIME_STAMP_COLUMN_NAME,\n",
    "    movie_id_column=ITEM_ID_COLUMN,\n",
    "    rating_column_name=RATING_COLUMN_NAME, \n",
    "    num_ratings_per_user=NUM_RATINGS_PER_USER, \n",
    "    num_similar_users=NUM_SIMILAR_USERS,\n",
    "    num_main_user_ratings=NUM_MAIN_USER_RATINGS,\n",
    "    test_selection_method='sequential',\n",
    "    save_path=CF_OUTPUT_TIMESTAMP_PATH, \n",
    "    seed=RANDOM_STATE\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate updated CF model predictions\n",
    "evaluate_model_predictions_rmse_mae(\n",
    "    data_path=CF_OUTPUT_TIMESTAMP_PATH,\n",
    "    num_examples=NUM_EXAMPLES,\n",
    "    actual_ratings_column='actual_rating',\n",
    "    predicted_ratings_column='predicted_rating'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CF_OUTPUT_RANDOM_MF_PATH = os.path.join(DATA_DIR, 'amazon-beauty/output/split_random/random_MF_CF_fewshot_.csv')\n",
    "print(f'Data path: {CF_OUTPUT_TIMESTAMP_PATH}')\n",
    "\n",
    "CF_RERUN_RANDOM_MF_PATH = os.path.join(DATA_DIR, 'amazon-beauty/output/split_random/rerun_random_MF_CF_fewshot.csv')\n",
    "print(f'Data path: {CF_RERUN_TIMESTAMP_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results_df = predict_ratings_with_CF_item_and_save(\n",
    "    data=data, \n",
    "    user_pcc_matrix=user_similarity_matrix, \n",
    "    item_pcc_matrix=item_similarity_matrix,\n",
    "    user_column_name=USER_COLUMN_NAME, \n",
    "    movie_column_name=TITLE_COLUMN_NAME, \n",
    "    timestamp_column_name=TIME_STAMP_COLUMN_NAME,\n",
    "    movie_id_column=ITEM_ID_COLUMN,\n",
    "    rating_column_name=RATING_COLUMN_NAME, \n",
    "    num_ratings_per_user=NUM_RATINGS_PER_USER, \n",
    "    num_similar_users=NUM_SIMILAR_USERS,\n",
    "    num_main_user_ratings=NUM_MAIN_USER_RATINGS,\n",
    "    test_selection_method='random',\n",
    "    save_path=CF_OUTPUT_RANDOM_MF_PATH, \n",
    "    seed=RANDOM_STATE\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate updated CF model predictions\n",
    "evaluate_model_predictions_rmse_mae(\n",
    "    data_path=CF_OUTPUT_RANDOM_MF_PATH,\n",
    "    num_examples=NUM_EXAMPLES,\n",
    "    actual_ratings_column='actual_rating',\n",
    "    predicted_ratings_column='predicted_rating'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Collaborative Filtering (NCF) \n",
    "\n",
    "NCF combines Matrix Factorization (MF) and Multi-Layer Perceptron (MLP) to learn user-item interaction patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# import theano\n",
    "# import theano.tensor as T\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializations\n",
    "from keras.regularizers import l1, l2, l1l2\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.layers import Embedding, Input, Dense, merge, Reshape, Merge, Flatten, Dropout\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from time import time\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "\n",
    "def get_NCF_model(num_users, num_items, mf_dim=10, layers=[10], reg_layers=[0], reg_mf=0):\n",
    "    assert len(layers) == len(reg_layers)\n",
    "    num_layer = len(layers) #Number of layers in the MLP\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "    \n",
    "    # Embedding layer\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = mf_dim, name = 'mf_embedding_user',\n",
    "                                  init = init_normal, W_regularizer = l2(reg_mf), input_length=1)\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = mf_dim, name = 'mf_embedding_item',\n",
    "                                  init = init_normal, W_regularizer = l2(reg_mf), input_length=1)   \n",
    "\n",
    "    MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = layers[0]/2, name = \"mlp_embedding_user\",\n",
    "                                  init = init_normal, W_regularizer = l2(reg_layers[0]), input_length=1)\n",
    "    MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = layers[0]/2, name = 'mlp_embedding_item',\n",
    "                                  init = init_normal, W_regularizer = l2(reg_layers[0]), input_length=1)   \n",
    "    \n",
    "    # MF part\n",
    "    mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    mf_vector = merge([mf_user_latent, mf_item_latent], mode = 'mul') # element-wise multiply\n",
    "\n",
    "    # MLP part \n",
    "    mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "    mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
    "    mlp_vector = merge([mlp_user_latent, mlp_item_latent], mode = 'concat')\n",
    "    for idx in xrange(1, num_layer):\n",
    "        layer = Dense(layers[idx], W_regularizer= l2(reg_layers[idx]), activation='relu', name=\"layer%d\" %idx)\n",
    "        mlp_vector = layer(mlp_vector)\n",
    "\n",
    "    # Concatenate MF and MLP parts\n",
    "    #mf_vector = Lambda(lambda x: x * alpha)(mf_vector)\n",
    "    #mlp_vector = Lambda(lambda x : x * (1-alpha))(mlp_vector)\n",
    "    predict_vector = merge([mf_vector, mlp_vector], mode = 'concat')\n",
    "    \n",
    "    # Final prediction layer\n",
    "    prediction = Dense(1, activation='sigmoid', init='lecun_uniform', name = \"prediction\")(predict_vector)\n",
    "    \n",
    "    model = Model(input=[user_input, item_input], \n",
    "                  output=prediction)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mf_dim = 8  # Dimensionality of MF embeddings\n",
    "layers = [64, 32, 16, 8]  # MLP layers\n",
    "reg_layers = [0, 0, 0, 0]  # Regularization for MLP layers\n",
    "reg_mf = 0  # Regularization for MF embeddings\n",
    "\n",
    "model = get_NCF_model(num_users, num_items, mf_dim, layers, reg_layers, reg_mf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "learning_rate = 0.001  # Learning rate\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "user_input, item_input, labels = get_train_instances(train, num_negatives)\n",
    "model.fit([np.array(user_input), np.array(item_input)], np.array(labels), epochs=num_epochs, batch_size=batch_size, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda install tensorflow --quiet\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def NCF(num_users, num_items, embedding_size, layers=[64, 32, 16, 8]):\n",
    "    # GMF part\n",
    "    user_input = Input(shape=(1,), name='user_input')\n",
    "    item_input = Input(shape=(1,), name='item_input')\n",
    "    \n",
    "    GMF_user_embedding = Embedding(output_dim=embedding_size, input_dim=num_users, input_length=1, name='gmf_user_embedding')(user_input)\n",
    "    GMF_item_embedding = Embedding(output_dim=embedding_size, input_dim=num_items, input_length=1, name='gmf_item_embedding')(item_input)\n",
    "    \n",
    "    GMF_user_flatten = Flatten()(GMF_user_embedding)\n",
    "    GMF_item_flatten = Flatten()(GMF_item_embedding)\n",
    "    GMF_multiply = Dot(axes=1)([GMF_user_flatten, GMF_item_flatten])\n",
    "    \n",
    "    # MLP part\n",
    "    MLP_user_embedding = Embedding(output_dim=layers[0] // 2, input_dim=num_users, input_length=1, name='mlp_user_embedding')(user_input)\n",
    "    MLP_item_embedding = Embedding(output_dim=layers[0] // 2, input_dim=num_items, input_length=1, name='mlp_item_embedding')(item_input)\n",
    "    \n",
    "    MLP_user_flatten = Flatten()(MLP_user_embedding)\n",
    "    MLP_item_flatten = Flatten()(MLP_item_embedding)\n",
    "    MLP_concat = Concatenate()([MLP_user_flatten, MLP_item_flatten])\n",
    "    \n",
    "    for idx in range(1, len(layers)):\n",
    "        layer = Dense(layers[idx], activation='relu', name=f'layer{idx}')(MLP_concat if idx == 1 else layer)\n",
    "    \n",
    "    # Concatenate GMF and MLP parts\n",
    "    concat = Concatenate()([GMF_multiply, layer])\n",
    "    output = Dense(1, activation='sigmoid', name='output')(concat)\n",
    "    \n",
    "    model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_embeddings_deep_learning(user_item_pairs, num_users, num_items, embedding_size=8):\n",
    "    # Assuming user_item_pairs is a list of tuples (user_id, item_id, interaction_flag)\n",
    "    user_ids = np.array([u for u, _, _ in user_item_pairs])\n",
    "    item_ids = np.array([i for _, i, _ in user_item_pairs])\n",
    "    labels = np.array([label for _, _, label in user_item_pairs])\n",
    "    \n",
    "    model = NCF(num_users, num_items, embedding_size)\n",
    "    \n",
    "    # Fit the model on your data\n",
    "    model.fit([user_ids, item_ids], labels, epochs=5, batch_size=64, verbose=1)\n",
    "    \n",
    "    # Extract embeddings\n",
    "    user_embedding_model = Model(inputs=model.input[0], outputs=model.get_layer('gmf_user_embedding').output)\n",
    "    item_embedding_model = Model(inputs=model.input[1], outputs=model.get_layer('gmf_item_embedding').output)\n",
    "    \n",
    "    user_embeddings = user_embedding_model.predict(np.arange(num_users))\n",
    "    item_embeddings = item_embedding_model.predict(np.arange(num_items))\n",
    "    \n",
    "    return user_embeddings, item_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract user and item IDs\n",
    "user_ids = data[USER_COLUMN_NAME].unique()\n",
    "item_ids = data[ITEM_ID_COLUMN].unique()\n",
    "num_users = len(user_ids)\n",
    "num_items = len(item_ids)\n",
    "\n",
    "# Map user and item IDs to their corresponding matrix indices\n",
    "user_id_to_index = {user_id: index for index, user_id in enumerate(user_ids)}\n",
    "item_id_to_index = {item_id: index for index, item_id in enumerate(item_ids)}\n",
    "\n",
    "# Generate interaction pairs for the get_embeddings_deep_learning function\n",
    "interaction_pairs = []\n",
    "for _, row in data.iterrows():\n",
    "    user_index = user_id_to_index[row[USER_COLUMN_NAME]]\n",
    "    item_index = item_id_to_index[row[ITEM_ID_COLUMN]]\n",
    "    rating = row[RATING_COLUMN_NAME]\n",
    "    interaction_pairs.append((user_index, item_index, rating))\n",
    "\n",
    "user_embeddings, item_embeddings = get_embeddings_deep_learning(interaction_pairs, num_users, num_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
