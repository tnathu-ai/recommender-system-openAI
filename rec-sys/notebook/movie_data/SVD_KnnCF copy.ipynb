{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt\n",
    "import openai\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from openai.error import RateLimitError\n",
    "import os\n",
    "import openai\n",
    " \n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset\n",
    "dataset_path = \"../data/ml-latest-small/ratings.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------> OBSERVATIONS:\n",
    "\n",
    "+ movieId: A unique identifier for the movie.\n",
    "+ title: The title of the movie, along with its release year in parentheses.\n",
    "+ genres: The genres associated with the movie, separated by pipe characters (|)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 610\n",
      "\n",
      "Number of unique movies: 9724\n",
      "\n",
      "Number of unique ratings: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unique users\n",
    "print(f'Number of unique users: {df.userId.unique().shape[0]}\\n')\n",
    "\n",
    "# unique movies\n",
    "print(f'Number of unique movies: {df.movieId.unique().shape[0]}\\n')\n",
    "\n",
    "# unique ratings\n",
    "print(f'Number of unique ratings: {df.rating.unique().shape[0]}\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Any infinities in the data with NaN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values and infinities\n",
    "df.isnull().sum()\n",
    "df.isnull().values.any()\n",
    "# check for infinities\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splits the data into a training set and a test set using a user-stratified train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M50 set:\n",
      "        userId  movieId  rating  timestamp\n",
      "560         6        2     4.0  845553522\n",
      "561         6        3     5.0  845554296\n",
      "562         6        4     3.0  845554349\n",
      "563         6        5     5.0  845553938\n",
      "564         6        6     4.0  845553757\n",
      "...       ...      ...     ...        ...\n",
      "97138     604      636     3.0  832080690\n",
      "97139     604      637     4.0  832081130\n",
      "97140     604      708     3.0  832080461\n",
      "97141     604      724     3.0  832080735\n",
      "97142     604      742     4.0  832080636\n",
      "\n",
      "[9119 rows x 4 columns]\n",
      "M100 set:\n",
      "        userId  movieId  rating   timestamp\n",
      "261         3       31     0.5  1306463578\n",
      "262         3      527     0.5  1306464275\n",
      "263         3      647     0.5  1306463619\n",
      "264         3      688     0.5  1306464228\n",
      "265         3      720     0.5  1306463595\n",
      "...       ...      ...     ...         ...\n",
      "96095     602      761     3.0   840876597\n",
      "96096     602      780     4.0   840876371\n",
      "96097     602      786     4.0   840876188\n",
      "96098     602      805     3.0   840876032\n",
      "96099     602      861     4.0   840876498\n",
      "\n",
      "[13159 rows x 4 columns]\n",
      "M400 set:\n",
      "         userId  movieId  rating   timestamp\n",
      "0            1        1     4.0   964982703\n",
      "1            1        3     4.0   964981247\n",
      "2            1        6     4.0   964982224\n",
      "3            1       47     5.0   964983815\n",
      "4            1       50     5.0   964982931\n",
      "...        ...      ...     ...         ...\n",
      "100831     610   166534     4.0  1493848402\n",
      "100832     610   168248     5.0  1493850091\n",
      "100833     610   168250     5.0  1494273047\n",
      "100834     610   168252     5.0  1493846352\n",
      "100835     610   170875     3.0  1493846415\n",
      "\n",
      "[67251 rows x 4 columns]\n",
      "Test set:\n",
      "        userId  movieId  rating   timestamp\n",
      "232         2      318     3.0  1445714835\n",
      "233         2      333     4.0  1445715029\n",
      "234         2     1704     4.5  1445715228\n",
      "235         2     3578     4.0  1445714885\n",
      "236         2     6874     4.0  1445714952\n",
      "...       ...      ...     ...         ...\n",
      "98474     606    76093     4.0  1368460114\n",
      "98475     606    91500     4.5  1349082477\n",
      "98476     606    95105     2.5  1349083032\n",
      "98477     606    97225     3.5  1368460191\n",
      "98478     606    98243     3.0  1368460316\n",
      "\n",
      "[11307 rows x 4 columns]\n",
      "Training set:\n",
      "         userId  movieId  rating   timestamp\n",
      "21452      140     4234     3.0  1012505945\n",
      "22899      156     2080     1.0   951113118\n",
      "58090      380   182639     4.0  1536874706\n",
      "79604      495     5254     3.5  1458636268\n",
      "100382     610    69134     3.0  1493848172\n",
      "...        ...      ...     ...         ...\n",
      "49818      318   136024     3.0  1455886225\n",
      "66934      432     5507     1.5  1315242710\n",
      "74191      474     4012     2.0  1046887657\n",
      "80857      510      497     1.5  1141158809\n",
      "40375      274    51709     3.5  1285897528\n",
      "\n",
      "[80668 rows x 4 columns]\n",
      "Test set:\n",
      "       userId  movieId  rating   timestamp\n",
      "0          1     1920     4.0   964981780\n",
      "1          1      457     5.0   964981909\n",
      "2          1     2648     4.0   964983414\n",
      "3          1      316     3.0   964982310\n",
      "4          1      661     5.0   964982838\n",
      "...      ...      ...     ...         ...\n",
      "3002     610    30745     5.0  1479545339\n",
      "3003     610   122882     5.0  1493845444\n",
      "3004     610     5953     4.0  1493849933\n",
      "3005     610     2161     4.0  1493879450\n",
      "3006     610    89582     4.5  1479545403\n",
      "\n",
      "[3007 rows x 4 columns]\n",
      "All-But-One Training set:\n",
      "         userId  movieId  rating   timestamp\n",
      "0            1        1     4.0   964982703\n",
      "1            1        3     4.0   964981247\n",
      "2            1        6     4.0   964982224\n",
      "3            1       47     5.0   964983815\n",
      "4            1       50     5.0   964982931\n",
      "...        ...      ...     ...         ...\n",
      "100831     610   166534     4.0  1493848402\n",
      "100832     610   168248     5.0  1493850091\n",
      "100833     610   168250     5.0  1494273047\n",
      "100834     610   168252     5.0  1493846352\n",
      "100835     610   170875     3.0  1493846415\n",
      "\n",
      "[100226 rows x 4 columns]\n",
      "All-But-One Test set:\n",
      "        userId  movieId  rating   timestamp\n",
      "219         1     3578     5.0   964980668\n",
      "245         2    77455     3.0  1445714941\n",
      "294         3     6835     5.0  1306463670\n",
      "306         4      106     4.0   986848784\n",
      "548         5      527     5.0   847434960\n",
      "...       ...      ...     ...         ...\n",
      "98227     606     7131     3.0  1171813499\n",
      "98506     607      423     3.0   963080410\n",
      "98707     608      188     3.5  1117503407\n",
      "99501     609      137     3.0   847221054\n",
      "99587     610      903     5.0  1479542931\n",
      "\n",
      "[610 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "def split_data_by_rated_items(df, test_size, given_n):\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=42, stratify=df['userId'])\n",
    "\n",
    "    # For each user in the test set, keep only 'given_n' rated items if they have rated that many,\n",
    "    # otherwise keep all the items they have rated.\n",
    "    test_df = test_df.groupby('userId').apply(lambda x: x.sample(min(len(x), given_n), random_state=42))\n",
    "\n",
    "    return train_df, test_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def split_data_by_unique_users(df):\n",
    "    unique_users = df['userId'].unique()\n",
    "    np.random.shuffle(unique_users)\n",
    "\n",
    "    # Get the user IDs for each set\n",
    "    M50_users = unique_users[:50]\n",
    "    M100_users = unique_users[50:150]\n",
    "    M400_users = unique_users[150:550]\n",
    "    test_users = unique_users[550:]\n",
    "\n",
    "    # Split the DataFrame into the different sets based on the user IDs\n",
    "    M50_df = df[df['userId'].isin(M50_users)]\n",
    "    M100_df = df[df['userId'].isin(M100_users)]\n",
    "    M400_df = df[df['userId'].isin(M400_users)]\n",
    "    test_df = df[df['userId'].isin(test_users)]\n",
    "\n",
    "    return M50_df, M100_df, M400_df, test_df\n",
    "\n",
    "\n",
    "def all_but_one(df):\n",
    "    # For each user, select one rating and split it into a separate DataFrame\n",
    "    test_df = df.groupby('userId').sample(n=1, random_state=42)\n",
    "    train_df = df.drop(test_df.index)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# Call the function\n",
    "M50_df, M100_df, M400_df, test_df = split_data_by_unique_users(df)\n",
    "\n",
    "print('M50 set:\\n', M50_df)\n",
    "print('M100 set:\\n', M100_df)\n",
    "print('M400 set:\\n', M400_df)\n",
    "print('Test set:\\n', test_df)\n",
    "\n",
    "# Call the functions\n",
    "train_df_given_10, test_df_given_10 = split_data_by_rated_items(df, test_size=0.2, given_n=5)\n",
    "print('Training set:\\n', train_df_given_10)\n",
    "print('Test set:\\n', test_df_given_10)\n",
    "\n",
    "train_df, test_df = all_but_one(df)\n",
    "print('All-But-One Training set:\\n', train_df)\n",
    "print('All-But-One Test set:\\n', test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "+ RMSE\n",
    "+ MAE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmse: root mean squared error\n",
    "# def rmse(y_true, y_pred):\n",
    "#     mse = mean_squared_error(y_true, y_pred)\n",
    "#     return sqrt(mse)\n",
    "\n",
    "# rmse from scratch\n",
    "def rmse(y_true, y_pred):\n",
    "    error = 0\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        error += (true - pred) ** 2\n",
    "    error /= len(y_true)\n",
    "    return sqrt(error)\n",
    "\n",
    "# mae: mean absolute error\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(np.array(y_true) - np.array(y_pred)))\n",
    "\n",
    "# f1 score: harmonic mean of precision and recall\n",
    "def f1_score(precisions, recalls):\n",
    "    f1_scores = dict()\n",
    "    for uid in precisions.keys():\n",
    "        p, r = precisions[uid], recalls[uid]\n",
    "        f1_scores[uid] = 2*(p*r) / (p + r) if (p + r) != 0 else 0\n",
    "    return f1_scores\n",
    "\n",
    "# Precision@k: https://github.com/RUCAIBox/RecBole/blob/master/recbole/evaluator/metrics.py\n",
    "def precision_at_k(predictions, k=10, threshold=3.5):\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        n_rel_and_rec_k = sum((true_r >= threshold) for (_, true_r) in user_ratings[:k])\n",
    "        precisions[uid] = n_rel_and_rec_k / k\n",
    "\n",
    "    return precisions\n",
    "\n",
    "# Recall@k: https://github.com/RUCAIBox/RecBole/blob/master/recbole/evaluator/metrics.py\n",
    "def recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "        n_rel_and_rec_k = sum((true_r >= threshold) for (_, true_r) in user_ratings[:k])\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "\n",
    "    return recalls\n",
    "\n",
    "# NDCG@k\n",
    "def ndcg_at_k(predictions, k=10):\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    ndcg_values = []\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        dcg = sum((rel / np.log2(ind + 2)) for ind, (est, rel) in enumerate(user_ratings[:k]))\n",
    "        idcg = sum((rel / np.log2(ind + 2)) for ind, (est, rel) in enumerate(sorted(user_ratings, key=lambda x: x[1], reverse=True)[:k]))\n",
    "        ndcg_values.append(dcg / idcg if idcg > 0.0 else 0.0)\n",
    "\n",
    "    return np.mean(ndcg_values)\n",
    "\n",
    "# evaluate function:\n",
    "def evaluate(predictions, k=10, threshold=3.5):\n",
    "    precisions = precision_at_k(predictions, k=k, threshold=threshold)\n",
    "    recalls = recall_at_k(predictions, k=k, threshold=threshold)\n",
    "    ndcg = ndcg_at_k(predictions, k=k)\n",
    "    \n",
    "    return {\n",
    "        'RMSE': rmse([true_r for uid, _, true_r, _ in predictions], [est for _, _, true_r, est in predictions]),\n",
    "        'MAE': mae([true_r for uid, _, true_r, _ in predictions], [est for _, _, true_r, est in predictions]),\n",
    "        'Precision@k': sum(prec for prec in precisions.values()) / len(precisions),\n",
    "        'Recall@k': sum(rec for rec in recalls.values()) / len(recalls),\n",
    "        'NDCG': ndcg,\n",
    "    }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD\n",
    "\n",
    "+ \"cold-start handling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD:\n",
    "    def __init__(self, num_factors, learning_rate, num_epochs, top_n=10):\n",
    "        # Initializing the instance variables with given arguments\n",
    "        self.num_factors = num_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.top_n = top_n  # number of movies to recommend for cold start\n",
    "\n",
    "    def fit(self, user_item_ratings):\n",
    "        # Initializing the user and movie latent factors matrices with random numbers\n",
    "        self.user_factors = np.random.randn(user_item_ratings.userId.nunique(), self.num_factors)\n",
    "        self.movie_factors = np.random.randn(user_item_ratings.movieId.nunique(), self.num_factors)\n",
    "        \n",
    "        # Creating dictionaries to map user and movie IDs to their respective indices in the factor matrices\n",
    "        self.user_index = {user_id: idx for idx, user_id in enumerate(user_item_ratings.userId.unique())}\n",
    "        self.movie_index = {movie_id: idx for idx, movie_id in enumerate(user_item_ratings.movieId.unique())}\n",
    "\n",
    "        # Calculate average rating for each movie\n",
    "        self.movie_avg_rating = user_item_ratings.groupby('movieId')['rating'].mean().to_dict()\n",
    "\n",
    "        # Get top-N movies based on average rating for cold start problem\n",
    "        sorted_movies_by_avg_rating = sorted(self.movie_avg_rating.items(), key=lambda x: x[1], reverse=True)\n",
    "        self.top_n_movies = [movie_id for movie_id, _ in sorted_movies_by_avg_rating[:self.top_n]]\n",
    "\n",
    "        # Loop over epochs\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # Loop over all user-item-rating rows in the DataFrame\n",
    "            for idx, row in user_item_ratings.iterrows():\n",
    "                user_id = row['userId']\n",
    "                movie_id = row['movieId']\n",
    "                rating = row['rating']\n",
    "\n",
    "                # Getting the user and movie indices for the current user-item pair\n",
    "                user_idx = self.user_index[user_id]\n",
    "                movie_idx = self.movie_index[movie_id]\n",
    "\n",
    "                # Computing the predicted rating as the dot product of the user and movie factors\n",
    "                prediction = np.dot(self.user_factors[user_idx], self.movie_factors[movie_idx])\n",
    "                # Computing the error as the difference between the actual and predicted ratings\n",
    "                error = rating - prediction\n",
    "\n",
    "                # Updating the user and movie factor vectors in the direction that minimizes the error\n",
    "                self.user_factors[user_idx] += self.learning_rate * error * self.movie_factors[movie_idx]\n",
    "                self.movie_factors[movie_idx] += self.learning_rate * error * self.user_factors[user_idx]\n",
    "\n",
    "    def predict(self, user_id, movie_id):\n",
    "        # Getting the user and movie indices for the given user-item pair\n",
    "        user_idx = self.user_index.get(user_id, -1)\n",
    "        movie_idx = self.movie_index.get(movie_id, -1)\n",
    "\n",
    "        # If the user or the movie is not present in the training data, return the movie's average rating\n",
    "        if user_idx == -1 or movie_idx == -1:\n",
    "            return self.movie_avg_rating.get(movie_id)\n",
    "\n",
    "        # Otherwise, return the predicted rating as the dot product of the user and movie factors\n",
    "        return np.dot(self.user_factors[user_idx], self.movie_factors[movie_idx])\n",
    "\n",
    "    def recommend(self, user_id):\n",
    "        # If the user is not present in the training data, return top-N movies\n",
    "        if user_id not in self.user_index:\n",
    "            return self.top_n_movies\n",
    "\n",
    "        # Otherwise, predict the rating for each movie and return the top-N movies\n",
    "        user_ratings = {movie_id: self.predict(user_id, movie_id) for movie_id in self.movie_index.keys()}\n",
    "        sorted_user_ratings = sorted(user_ratings.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [movie_id for movie_id, _ in sorted_user_ratings[:self.top_n]]\n",
    "    \n",
    "\n",
    "svd = SVD(num_factors=35, learning_rate=0.01, num_epochs=10, top_n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(df, model):\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    # Fit the model to the data\n",
    "    model.fit(df)\n",
    "\n",
    "    # Predict ratings for the Test set and evaluate\n",
    "    test_predictions = test_df.apply(lambda row: model.predict(row['userId'], row['movieId']), axis=1)\n",
    "    \n",
    "    # Remove None values and corresponding actual ratings\n",
    "    actual_ratings = test_df['rating'][test_predictions.notna()]\n",
    "    test_predictions = test_predictions.dropna()\n",
    "\n",
    "    svd_predictions = [(uid, iid, true_r, est) for uid, iid, true_r, est in zip(test_df['userId'], test_df['movieId'], actual_ratings, test_predictions)]\n",
    "    \n",
    "    # Compute metrics for the model\n",
    "    metrics = evaluate(svd_predictions)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Execution time for: {end_time - start_time} seconds\")\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for M50 dataset\n",
      "Execution time for: 3.3885159492492676 seconds\n",
      "The evaluation metrics for the SVD model are: {'RMSE': 1.045812000255495, 'MAE': 0.7969807332133018, 'Precision@k': 0.06729323308270721, 'Recall@k': 1.0, 'NDCG': 1.0}\n",
      "\n",
      "Execution time for M100 dataset\n"
     ]
    }
   ],
   "source": [
    "print(f\"Execution time for M50 dataset\")\n",
    "svd_metrics_M50 = evaluate_model(M50_df, svd)\n",
    "print(f\"The evaluation metrics for the SVD model are: {svd_metrics_M50}\\n\")\n",
    "\n",
    "print(f\"Execution time for M100 dataset\")\n",
    "svd_metrics_M100 = evaluate_model(M100_df, svd)\n",
    "print(f\"The evaluation metrics for the SVD model are: {svd_metrics_M100}\\n\")\n",
    "\n",
    "print(f\"Execution time for M400 dataset\")\n",
    "svd_metrics_M400 = evaluate_model(M400_df, svd)\n",
    "print(f\"The evaluation metrics for the SVD model are: {svd_metrics_M400}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Convert the metrics to DataFrame\n",
    "df_M50 = pd.DataFrame([svd_metrics_M50])\n",
    "df_M50['Dataset'] = 'M50'\n",
    "\n",
    "df_M100 = pd.DataFrame([svd_metrics_M100])\n",
    "df_M100['Dataset'] = 'M100'\n",
    "\n",
    "df_M400 = pd.DataFrame([svd_metrics_M400])\n",
    "df_M400['Dataset'] = 'M400'\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "metrics_df = pd.concat([df_M50, df_M100, df_M400], ignore_index=True)\n",
    "\n",
    "# Reorder the columns\n",
    "cols = metrics_df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]  # Move the last column to first\n",
    "metrics_df = metrics_df[cols]\n",
    "\n",
    "metrics_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN based CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_CF:\n",
    "    def __init__(self, n_users, n_items, k=3, gamma=0, delta=25, epsilon=1e-9):\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.k = k\n",
    "        self.gamma = gamma\n",
    "        self.delta = delta\n",
    "        self.epsilon = epsilon\n",
    "        self.user_corrs = np.zeros((n_users, n_users))\n",
    "        self.item_corrs = np.zeros((n_items, n_items))\n",
    "\n",
    "    def fit(self, user_item_matrix):\n",
    "        # user-based\n",
    "        for i in range(self.n_users):\n",
    "            for j in range(self.n_users):\n",
    "                self.user_corrs[i, j] = self.pearson_corr(user_item_matrix[i], user_item_matrix[j])\n",
    "\n",
    "        # item-based\n",
    "        for i in range(self.n_items):\n",
    "            for j in range(self.n_items):\n",
    "                self.item_corrs[i, j] = self.pearson_corr(user_item_matrix[:, i], user_item_matrix[:, j])\n",
    "\n",
    "    def predict(self, user_item_matrix, mode='user'):\n",
    "        predictions = np.zeros((self.n_users, self.n_items))\n",
    "        if mode == 'user':\n",
    "            for i in range(self.n_users):\n",
    "                for j in range(self.n_items):\n",
    "                    if user_item_matrix[i, j] > 0:\n",
    "                        sim_users = np.argsort(self.user_corrs[i])[-(self.k + 1):-1]\n",
    "                        predictions[i, j] = self.predict_rating(user_item_matrix, sim_users, i, j, mode)\n",
    "        elif mode == 'item':\n",
    "            for i in range(self.n_users):\n",
    "                for j in range(self.n_items):\n",
    "                    if user_item_matrix[i, j] > 0:\n",
    "                        sim_items = np.argsort(self.item_corrs[j])[-(self.k + 1):-1]\n",
    "                        predictions[i, j] = self.predict_rating(user_item_matrix, sim_items, i, j, mode)\n",
    "        return predictions\n",
    "\n",
    "    def pearson_corr(self, vec_i, vec_j):\n",
    "        mask_i = vec_i > 0\n",
    "        mask_j = vec_j > 0\n",
    "        corrated_index = np.intersect1d(np.where(mask_i), np.where(mask_j))\n",
    "        if len(corrated_index) == 0:\n",
    "            return 0\n",
    "        mean_i = np.mean(vec_i[corrated_index])\n",
    "        mean_j = np.mean(vec_j[corrated_index])\n",
    "        sub_i = vec_i[corrated_index] - mean_i\n",
    "        sub_j = vec_j[corrated_index] - mean_j\n",
    "        return np.sum(sub_i * sub_j) / (np.sqrt(np.sum(np.square(sub_i))) * np.sqrt(np.sum(np.square(sub_j))) + self.epsilon)\n",
    "\n",
    "    def predict_rating(self, user_item_matrix, sim_indices, i, j, mode):\n",
    "        if mode == 'user':\n",
    "            sim_ratings = user_item_matrix[sim_indices, j]\n",
    "            sim_means = np.array([np.mean(user_item_matrix[k][user_item_matrix[k]>0]) for k in sim_indices])\n",
    "            sim_vals = self.user_corrs[i][sim_indices]\n",
    "        elif mode == 'item':\n",
    "            sim_ratings = user_item_matrix[i, sim_indices]\n",
    "            sim_means = np.array([np.mean(user_item_matrix[:, k][user_item_matrix[:, k]>0]) for k in sim_indices])\n",
    "            sim_vals = self.item_corrs[j][sim_indices]\n",
    "        if np.sum(sim_vals) == 0:\n",
    "            return np.mean(sim_ratings)\n",
    "        else:\n",
    "            return np.mean(sim_ratings) + np.sum(sim_vals * (sim_ratings - sim_means)) / np.sum(sim_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_matrix(df, nrows, ncols):\n",
    "    matrix = np.zeros((nrows, ncols))\n",
    "    for row in df.itertuples():\n",
    "        matrix[row.userId, row.movieId] = row.rating\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create mappings for userIds and movieIds to contiguous indices\n",
    "user_mapping = {user_id: i for i, user_id in enumerate(M100_df['userId'].unique())}\n",
    "movie_mapping = {movie_id: i for i, movie_id in enumerate(M100_df['movieId'].unique())}\n",
    "\n",
    "# Create reverse mappings for later use\n",
    "reverse_user_mapping = {i: user_id for user_id, i in user_mapping.items()}\n",
    "reverse_movie_mapping = {i: movie_id for movie_id, i in movie_mapping.items()}\n",
    "\n",
    "# Apply the mappings to the dataframes\n",
    "M100_df['userId'] = M100_df['userId'].map(user_mapping)\n",
    "M100_df['movieId'] = M100_df['movieId'].map(movie_mapping)\n",
    "\n",
    "test_df['userId'] = test_df['userId'].map(user_mapping)\n",
    "test_df['movieId'] = test_df['movieId'].map(movie_mapping)\n",
    "\n",
    "# Drop rows with NaN userId or movieId\n",
    "test_df.dropna(subset=['userId', 'movieId'], inplace=True)\n",
    "\n",
    "# Convert userId and movieId to integer\n",
    "test_df['userId'] = test_df['userId'].astype(int)\n",
    "test_df['movieId'] = test_df['movieId'].astype(int)\n",
    "\n",
    "\n",
    "n_users = M100_df['userId'].nunique()\n",
    "n_items = M100_df['movieId'].nunique()\n",
    "\n",
    "train_matrix = df_to_matrix(M100_df, n_users, n_items)\n",
    "test_matrix = df_to_matrix(test_df, n_users, n_items)\n",
    "\n",
    "knn_cf = KNN_CF(n_users, n_items, k=3)\n",
    "\n",
    "# Fit the model to the M100 data\n",
    "knn_cf.fit(train_matrix)\n",
    "\n",
    "# Predict ratings for the Test set and evaluate\n",
    "user_based_predictions = knn_cf.predict(test_matrix, mode='user')\n",
    "test_predictions = user_based_predictions[test_matrix.nonzero()]\n",
    "actual_ratings = test_matrix[test_matrix.nonzero()]\n",
    "\n",
    "knn_predictions = [(uid, iid, true_r, est) for uid, iid, true_r, est in zip(test_df['userId'], test_df['movieId'], actual_ratings, test_predictions)]\n",
    "# Compute metrics for the KNN model\n",
    "knn_metrics_M100 = evaluate(knn_predictions)\n",
    "\n",
    "# create a dataframe for the results \n",
    "knn_results = pd.DataFrame(knn_metrics_M100, index=[0])\n",
    "# add first column of the dataframe as the dataset name\n",
    "knn_results.insert(0, 'dataset', 'M100')\n",
    "knn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Create mappings for userIds and movieIds to contiguous indices\n",
    "user_mapping = {user_id: i for i, user_id in enumerate(M50_df['userId'].unique())}\n",
    "movie_mapping = {movie_id: i for i, movie_id in enumerate(M50_df['movieId'].unique())}\n",
    "\n",
    "# Create reverse mappings for later use\n",
    "reverse_user_mapping = {i: user_id for user_id, i in user_mapping.items()}\n",
    "reverse_movie_mapping = {i: movie_id for movie_id, i in movie_mapping.items()}\n",
    "\n",
    "# Apply the mappings to the dataframes\n",
    "M50_df['userId'] = M50_df['userId'].map(user_mapping)\n",
    "M50_df['movieId'] = M50_df['movieId'].map(movie_mapping)\n",
    "\n",
    "test_df['userId'] = test_df['userId'].map(user_mapping)\n",
    "test_df['movieId'] = test_df['movieId'].map(movie_mapping)\n",
    "\n",
    "# Drop rows with NaN userId or movieId\n",
    "test_df.dropna(subset=['userId', 'movieId'], inplace=True)\n",
    "\n",
    "# Convert userId and movieId to integer\n",
    "test_df['userId'] = test_df['userId'].astype(int)\n",
    "test_df['movieId'] = test_df['movieId'].astype(int)\n",
    "\n",
    "\n",
    "n_users = M50_df['userId'].nunique()\n",
    "n_items = M50_df['movieId'].nunique()\n",
    "\n",
    "train_matrix = df_to_matrix(M50_df, n_users, n_items)\n",
    "test_matrix = df_to_matrix(test_df, n_users, n_items)\n",
    "\n",
    "knn_cf = KNN_CF(n_users, n_items, k=3)\n",
    "\n",
    "# Fit the model to the M100 data\n",
    "knn_cf.fit(train_matrix)\n",
    "\n",
    "# Predict ratings for the Test set and evaluate\n",
    "user_based_predictions = knn_cf.predict(test_matrix, mode='user')\n",
    "test_predictions = user_based_predictions[test_matrix.nonzero()]\n",
    "actual_ratings = test_matrix[test_matrix.nonzero()]\n",
    "\n",
    "knn_predictions = [(uid, iid, true_r, est) for uid, iid, true_r, est in zip(test_df['userId'], test_df['movieId'], actual_ratings, test_predictions)]\n",
    "# Compute metrics for the KNN model\n",
    "knn_results_M50 = evaluate(knn_predictions)\n",
    "\n",
    "# create a dataframe to concatenate the results\n",
    "knn_results_M50 = pd.DataFrame(knn_results_M50, index=[0])\n",
    "# add first column of the dataframe as the dataset name\n",
    "knn_results_M50.insert(0, 'dataset', 'M50')\n",
    "knn_results_M50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    if len(y_true) == 0:\n",
    "        raise ValueError(\"y_true is empty.\")\n",
    "    error = 0\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        error += (true - pred) ** 2\n",
    "    error /= len(y_true)\n",
    "    return sqrt(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Create mappings for userIds and movieIds to contiguous indices\n",
    "user_mapping = {user_id: i for i, user_id in enumerate(M400_df['userId'].unique())}\n",
    "movie_mapping = {movie_id: i for i, movie_id in enumerate(M400_df['movieId'].unique())}\n",
    "\n",
    "# Create reverse mappings for later use\n",
    "reverse_user_mapping = {i: user_id for user_id, i in user_mapping.items()}\n",
    "reverse_movie_mapping = {i: movie_id for movie_id, i in movie_mapping.items()}\n",
    "\n",
    "# Apply the mappings to the dataframes\n",
    "M400_df['userId'] = M400_df['userId'].map(user_mapping)\n",
    "M400_df['movieId'] = M400_df['movieId'].map(movie_mapping)\n",
    "\n",
    "test_df['userId'] = test_df['userId'].map(user_mapping)\n",
    "test_df['movieId'] = test_df['movieId'].map(movie_mapping)\n",
    "\n",
    "# Drop rows with NaN userId or movieId\n",
    "test_df.dropna(subset=['userId', 'movieId'], inplace=True)\n",
    "\n",
    "# Convert userId and movieId to integer\n",
    "test_df['userId'] = test_df['userId'].astype(int)\n",
    "test_df['movieId'] = test_df['movieId'].astype(int)\n",
    "\n",
    "\n",
    "n_users = M400_df['userId'].nunique()\n",
    "n_items = M400_df['movieId'].nunique()\n",
    "\n",
    "train_matrix = df_to_matrix(M400_df, n_users, n_items)\n",
    "test_matrix = df_to_matrix(test_df, n_users, n_items)\n",
    "\n",
    "knn_cf = KNN_CF(n_users, n_items, k=3)\n",
    "\n",
    "# Fit the model to the M100 data\n",
    "knn_cf.fit(train_matrix)\n",
    "\n",
    "# Predict ratings for the Test set and evaluate\n",
    "user_based_predictions = knn_cf.predict(test_matrix, mode='user')\n",
    "test_predictions = user_based_predictions[test_matrix.nonzero()]\n",
    "actual_ratings = test_matrix[test_matrix.nonzero()]\n",
    "\n",
    "knn_predictions = [(uid, iid, true_r, est) for uid, iid, true_r, est in zip(test_df['userId'], test_df['movieId'], actual_ratings, test_predictions)]\n",
    "# Compute metrics for the KNN model\n",
    "knn_metrics_M100 = evaluate(knn_predictions)\n",
    "\n",
    "# create a dataframe to concatenate the results\n",
    "knn_results_M400 = pd.DataFrame(knn_metrics_M100, index=[0])\n",
    "# add first column of the dataframe as the dataset name\n",
    "knn_results_M400.insert(0, 'dataset', 'M400')\n",
    "knn_results_M400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'surprise'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msurprise\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset, Reader, SVD, KNNBasic, accuracy\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msurprise\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m      5\u001b[0m \u001b[39m# 1. Preprocess the data\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'surprise'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, SVD, KNNBasic, accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# 1. Preprocess the data\n",
    "reader = Reader(rating_scale=(0.5, 5))\n",
    "data = Dataset.load_from_df(df[['userId', 'movieId', 'rating']], reader)\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# 2. Implement and evaluate SVD\n",
    "svd_model = SVD()\n",
    "svd_model.fit(trainset)\n",
    "svd_predictions = svd_model.test(testset)\n",
    "\n",
    "svd_rmse = accuracy.rmse(svd_predictions)\n",
    "svd_mae = accuracy.mae(svd_predictions)\n",
    "\n",
    "# 3. Implement and evaluate KNN-based collaborative filtering\n",
    "knn_model = KNNBasic()\n",
    "knn_model.fit(trainset)\n",
    "knn_predictions = knn_model.test(testset)\n",
    "\n",
    "knn_rmse = accuracy.rmse(knn_predictions)\n",
    "knn_mae = accuracy.mae(knn_predictions)\n",
    "\n",
    "# 4. Evaluate both models and present the results in a DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['SVD', 'KNN-based CF'],\n",
    "    'RMSE': [svd_rmse, knn_rmse],\n",
    "    'MAE': [svd_mae, knn_mae]\n",
    "})\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
