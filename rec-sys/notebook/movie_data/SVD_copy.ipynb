{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD from scratch using gradient descent\n",
    "\n",
    "uses the described backpropagation algorithm for gradient descent and singular vectors as described in the [article](https://sifter.org/simon/journal/20070815.html)\n",
    "\n",
    "1. Extract user-item interactions from the ratings dataframe.\n",
    "2. Define the SVD model with functions for initializing the user and movie vectors, predicting ratings, and updating the vectors using gradient descent.\n",
    "3. Train the model on the user-item interactions data.\n",
    "4. Use the learned vectors to make predictions on new user-movie pairs.\n",
    "\n",
    "## Data\n",
    "\n",
    "This dataset (ml-latest-small) describes 5-star rating and free-text tagging activity from [MovieLens](http://movielens.org), a movie recommendation service. It contains 100836 ratings and 3683 tag applications across 9742 movies. These data were created by 610 users between March 29, 1996 and September 24, 2018. This dataset was generated on September 26, 2018.\n",
    "\n",
    "Users were selected at random for inclusion. All selected users had rated at least 20 movies. No demographic information is included. Each user is represented by an id, and no other information is provided.\n",
    "\n",
    "The data are contained in the files `links.csv`, `movies.csv`, `ratings.csv` and `tags.csv`. More details about the contents and use of all these files follows.\n",
    "\n",
    "This is a *development* dataset. As such, it may change over time and is not an appropriate dataset for shared research results. See available *benchmark* datasets if that is your intent.\n",
    "\n",
    "This and other GroupLens data sets are publicly available for download at <http://grouplens.org/datasets/>.\n",
    "\n",
    "\n",
    "## Evaluation metric: \n",
    "+ RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 610\n",
      "\n",
      "Number of unique movies: 9724\n",
      "\n",
      "Number of unique ratings: 10\n",
      "Number of unique ratings: [4.  5.  3.  2.  1.  4.5 3.5 2.5 0.5 1.5]\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100836 entries, 0 to 100835\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   userId     100836 non-null  int64  \n",
      " 1   movieId    100836 non-null  int64  \n",
      " 2   rating     100836 non-null  float64\n",
      " 3   timestamp  100836 non-null  int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 3.1 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df = pd.read_csv('../data/ml-latest-small/ratings.csv')\n",
    "\n",
    "# unique users\n",
    "print(f'Number of unique users: {ratings_df.userId.unique().shape[0]}\\n')\n",
    "\n",
    "# unique movies\n",
    "print(f'Number of unique movies: {ratings_df.movieId.unique().shape[0]}\\n')\n",
    "\n",
    "# unique ratings\n",
    "print(f'Number of unique ratings: {ratings_df.rating.unique().shape[0]}')\n",
    "# unique ratings\n",
    "print(f'Number of unique ratings: {ratings_df.rating.unique()}\\n')\n",
    "\n",
    "ratings_df.info()\n",
    "ratings_df.head(3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------> OBSERVATIONS:\n",
    "\n",
    "+ userId: A unique identifier for the user who provided the rating.\n",
    "+ movieId: A unique identifier for the movie, which is consistent with the movieId in the movies.csv file.\n",
    "+ rating: The user's rating for the movie on a scale of 0.5 to 5 stars, given in increments of 0.5 stars.\n",
    "+ timestamp: The Unix timestamp representing the time at which the user provided the rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_rated_items_unique_users(df, test_size, given_n):\n",
    "\n",
    "    # Extract user-item interactions from the ratings DataFrame\n",
    "    df = df[['userId', 'movieId', 'rating']]\n",
    "\n",
    "    unique_users = df['userId'].unique()\n",
    "    np.random.shuffle(unique_users)\n",
    "\n",
    "    # Get the user IDs for each set\n",
    "    # the size of each set is 50, 100, 400, and 60 users respectively\n",
    "    M50_users = unique_users[:50]\n",
    "    M100_users = unique_users[50:150]\n",
    "    M400_users = unique_users[150:550]\n",
    "    test_users = unique_users[550:]\n",
    "\n",
    "    # Split the DataFrame into the different sets based on the user IDs\n",
    "    M50_df = df[df['userId'].isin(M50_users)]\n",
    "    M100_df = df[df['userId'].isin(M100_users)]\n",
    "    M400_df = df[df['userId'].isin(M400_users)]\n",
    "    test_df = df[df['userId'].isin(test_users)]\n",
    "\n",
    "    # For each user in the test set, keep only 'given_n' rated items if they have rated that many,\n",
    "    # otherwise keep all the items they have rated.\n",
    "    test_df = test_df.groupby('userId').apply(lambda x: x.sample(min(len(x), given_n), random_state=42))\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "    return M50_df, M100_df, M400_df, test_df\n",
    "\n",
    "\n",
    "def all_but_one(df):\n",
    "    # For each user, select one rating and split it into a separate DataFrame\n",
    "    test_df = df.groupby('userId').sample(n=1, random_state=42)\n",
    "    train_df = df.drop(test_df.index)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "# Call the functions\n",
    "M50_df, M100_df, M400_df, test_df_given_10 = split_data_by_rated_items_unique_users(ratings_df, test_size=0.2, given_n=10)\n",
    "# print('Training set:\\n', train_df_given_10)\n",
    "# print('Test set:\\n', test_df_given_10)\n",
    "\n",
    "train_df, test_df = all_but_one(ratings_df)\n",
    "# print('All-But-One Training set:\\n', train_df)\n",
    "# print('All-But-One Test set:\\n', test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmse: root mean squared error\n",
    "def rmse(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    return sqrt(mse)\n",
    "\n",
    "# mae: mean absolute error\n",
    "def mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(np.array(y_true) - np.array(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SVD (Singular Value Decomposition) model class\n",
    "class SVD:\n",
    "    def __init__(self, num_factors, learning_rate, num_epochs):\n",
    "        # Initialize model parameters: number of factors (dimensionality of the user/item embeddings),\n",
    "        # learning rate for stochastic gradient descent, and number of epochs for training\n",
    "        self.num_factors = num_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def fit(self, user_item_ratings):\n",
    "        # Initialize user and movie factors as random matrices with dimensions (# of unique users/items) x (num_factors)\n",
    "        self.user_factors = np.random.randn(user_item_ratings.userId.nunique(), self.num_factors)\n",
    "        self.movie_factors = np.random.randn(user_item_ratings.movieId.nunique(), self.num_factors)\n",
    "        \n",
    "        # Create dictionaries mapping user/movie IDs to their corresponding indices in the factor matrices\n",
    "        self.user_index = {user_id: idx for idx, user_id in enumerate(user_item_ratings.userId.unique())}\n",
    "        self.movie_index = {movie_id: idx for idx, movie_id in enumerate(user_item_ratings.movieId.unique())}\n",
    "\n",
    "        # Iterate through the data num_epochs times\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # Iterate over all user-movie interactions in the data\n",
    "            for _, (user_id, movie_id, rating) in user_item_ratings.iterrows():\n",
    "                # Get user/movie indices\n",
    "                user_idx = self.user_index[user_id]\n",
    "                movie_idx = self.movie_index[movie_id]\n",
    "\n",
    "                # Predict the rating and calculate the prediction error\n",
    "                prediction = np.dot(self.user_factors[user_idx], self.movie_factors[movie_idx])\n",
    "                error = rating - prediction\n",
    "\n",
    "                # Update user and movie factors using the calculated error, and the learning rate\n",
    "                self.user_factors[user_idx] += self.learning_rate * error * self.movie_factors[movie_idx]\n",
    "                self.movie_factors[movie_idx] += self.learning_rate * error * self.user_factors[user_idx]\n",
    "\n",
    "    def predict(self, user_id, movie_id):\n",
    "        # Get user/movie indices\n",
    "        user_idx = self.user_index.get(user_id, -1)\n",
    "        movie_idx = self.movie_index.get(movie_id, -1)\n",
    "\n",
    "        # If the user or movie is unknown, return None\n",
    "        if user_idx == -1 or movie_idx == -1:\n",
    "            return None\n",
    "\n",
    "        # Predict the rating as the dot product of user and movie factors\n",
    "        return np.dot(self.user_factors[user_idx], self.movie_factors[movie_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:16\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[1;32m/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD copy 2.ipynb Cell 8\u001b[0m in \u001b[0;36mrmse\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy%202.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrmse\u001b[39m(y_true, y_pred):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy%202.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     mse \u001b[39m=\u001b[39m mean_squared_error(y_true, y_pred)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD%20copy%202.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m sqrt(mse)\n",
      "File \u001b[0;32m~/.conda/envs/Advanced_Programming_for_Data_Science/lib/python3.10/site-packages/sklearn/metrics/_regression.py:438\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmean_squared_error\u001b[39m(\n\u001b[1;32m    379\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, multioutput\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39muniform_average\u001b[39m\u001b[39m\"\u001b[39m, squared\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    380\u001b[0m ):\n\u001b[1;32m    381\u001b[0m     \u001b[39m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \n\u001b[1;32m    383\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[39m    0.825...\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 438\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[39m=\u001b[39m _check_reg_targets(\n\u001b[1;32m    439\u001b[0m         y_true, y_pred, multioutput\n\u001b[1;32m    440\u001b[0m     )\n\u001b[1;32m    441\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    442\u001b[0m     output_errors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage((y_true \u001b[39m-\u001b[39m y_pred) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, weights\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/.conda/envs/Advanced_Programming_for_Data_Science/lib/python3.10/site-packages/sklearn/metrics/_regression.py:95\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39m    the dtype argument passed to check_array.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     94\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[0;32m---> 95\u001b[0m y_true \u001b[39m=\u001b[39m check_array(y_true, ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m     96\u001b[0m y_pred \u001b[39m=\u001b[39m check_array(y_pred, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m     98\u001b[0m \u001b[39mif\u001b[39;00m y_true\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/Advanced_Programming_for_Data_Science/lib/python3.10/site-packages/sklearn/utils/validation.py:805\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    803\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n\u001b[1;32m    804\u001b[0m     \u001b[39mif\u001b[39;00m n_samples \u001b[39m<\u001b[39m ensure_min_samples:\n\u001b[0;32m--> 805\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    806\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m sample(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    807\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    808\u001b[0m             \u001b[39m%\u001b[39m (n_samples, array\u001b[39m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m    809\u001b[0m         )\n\u001b[1;32m    811\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_features \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    812\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train the model on the train set\n",
    "model_SVD_M50_df = SVD(num_factors=35, learning_rate=0.001, num_epochs=100)\n",
    "model_SVD_M50_df.fit(M50_df)\n",
    "\n",
    "# Predict and evaluate on the test set\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for _, (user_id, movie_id, rating) in test_df_given_10.iterrows():\n",
    "    prediction = model_SVD_M50_df.predict(user_id, movie_id)\n",
    "    \n",
    "    if prediction is not None:\n",
    "        y_true.append(rating)\n",
    "        y_pred.append(prediction)\n",
    "\n",
    "rmse_value = rmse(y_true, y_pred)\n",
    "print(f\"Root Mean Squared Error: {rmse_value}\")\n",
    "print(f\"Mean Absolute Error: {mae(y_true, y_pred)}\")\n",
    "\n",
    "# export model into model folder\n",
    "pickle.dump(model_SVD_M50_df, open('../models/model_SVD_M50_df.pkl', 'wb'))\n",
    "print('model_SVD_M50_df.pkl exported')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----------> OBSERVATIONS: \n",
    "\n",
    "The model's Root Mean Squared Error (RMSE) is 2.097, and the Mean Absolute Error (MAE) is 1.349. These values indicate that on average, the model's predictions are approximately 2.1 and 1.35 rating points off from the actual ratings, respectively. Given the rating scale of 0.5 to 5.0, these errors are relatively high, suggesting that the model's accuracy could be improved. The total computation time was approximately 4 minutes and 55 seconds."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Based CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_CF:\n",
    "    def __init__(self, k, user_based=True):\n",
    "        self.k = k\n",
    "        self.user_based = user_based\n",
    "        self.EPSILON = 1e-9\n",
    "\n",
    "    def fit(self, user_item_ratings):\n",
    "        self.user_item_matrix = user_item_ratings.pivot(index='userId', columns='movieId', values='rating').fillna(0).values\n",
    "        self.n_users, self.n_items = self.user_item_matrix.shape\n",
    "        self.user_ids = user_item_ratings['userId'].unique()\n",
    "        self.item_ids = user_item_ratings['movieId'].unique()\n",
    "\n",
    "        # Compute Pearson Correlation Coefficient for Each Pair\n",
    "        if self.user_based:\n",
    "            self.matrix = self.user_item_matrix\n",
    "            self.n_entities = self.n_users\n",
    "        else:\n",
    "            self.matrix = self.user_item_matrix.T\n",
    "            self.n_entities = self.n_items\n",
    "\n",
    "        self.pearson_corr = np.zeros((self.n_entities, self.n_entities))\n",
    "        for i in range(self.n_entities):\n",
    "            for j in range(self.n_entities):\n",
    "                mask_i = self.matrix[i, :] > 0\n",
    "                mask_j = self.matrix[j, :] > 0\n",
    "                corrated_index = np.intersect1d(np.where(mask_i), np.where(mask_j))\n",
    "                if len(corrated_index) == 0:\n",
    "                    continue\n",
    "                mean_i = np.sum(self.matrix[i, :]) / (np.sum(np.clip(self.matrix[i, :], 0, 1)) + self.EPSILON)\n",
    "                mean_j = np.sum(self.matrix[j, :]) / (np.sum(np.clip(self.matrix[j, :], 0, 1)) + self.EPSILON)\n",
    "                i_sub_mean = self.matrix[i, corrated_index] - mean_i\n",
    "                j_sub_mean = self.matrix[j, corrated_index] - mean_j\n",
    "                sum_sqrt_i = np.sqrt(np.sum(np.square(i_sub_mean)))\n",
    "                sum_sqrt_j = np.sqrt(np.sum(np.square(j_sub_mean)))\n",
    "                sim = np.sum(i_sub_mean * j_sub_mean) / (sum_sqrt_i * sum_sqrt_j + self.EPSILON)\n",
    "                self.pearson_corr[i, j] = sim\n",
    "\n",
    "    def predict(self, user_id, movie_id):\n",
    "        if user_id in self.user_ids and movie_id in self.item_ids:\n",
    "            user_idx = np.where(self.user_ids == user_id)[0][0]\n",
    "            item_idx = np.where(self.item_ids == movie_id)[0][0]\n",
    "            \n",
    "            # User-based CF\n",
    "            if self.user_based:\n",
    "                sim_entity_indices = np.argsort(self.pearson_corr[user_idx])[-(self.k + 1):-1]\n",
    "                sim_vals = self.pearson_corr[user_idx, sim_entity_indices]\n",
    "                entity_mean = np.sum(self.matrix[user_idx, :]) / (np.sum(np.clip(self.matrix[user_idx, :], 0, 1)) + self.EPSILON)\n",
    "                sim_entities = self.matrix[sim_entity_indices, :]\n",
    "                mask_rated = sim_entities[:, item_idx] > 0\n",
    "                sim_vals = sim_vals[mask_rated]\n",
    "                sim_entities = sim_entities[mask_rated, :]\n",
    "                \n",
    "            # Item-based CF\n",
    "            else:\n",
    "                sim_entity_indices = np.argsort(self.pearson_corr[item_idx])[-(self.k + 1):-1]\n",
    "                sim_vals = self.pearson_corr[item_idx, sim_entity_indices]\n",
    "                entity_mean = np.sum(self.matrix[:, user_idx]) / (np.sum(np.clip(self.matrix[:, user_idx], 0, 1)) + self.EPSILON)\n",
    "                sim_entities = self.matrix[:, sim_entity_indices]\n",
    "                mask_rated = sim_entities[user_idx, :] > 0\n",
    "                sim_vals = sim_vals[mask_rated]\n",
    "                sim_entities = sim_entities[:, mask_rated]\n",
    "\n",
    "            sim_entity_mean = np.sum(sim_entities, axis=1) / (np.sum(np.clip(sim_entities, 0, 1), axis=1) + self.EPSILON)\n",
    "            sim_r_sum_mean = sim_vals * (sim_entities - sim_entity_mean[:, None])\n",
    "            \n",
    "            prediction = entity_mean + np.sum(sim_r_sum_mean) / (np.sum(sim_vals) + self.EPSILON)\n",
    "            prediction = np.clip(prediction, 0, 5)\n",
    "            \n",
    "            return prediction\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-Based Collaborative Filtering\n",
    "\n",
    "the model finds users that are similar to the target user based on their rating history. It then predicts the target user's rating for a specific item based on the ratings given to that item by the similar users.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (0,) (0,9724) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:10\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[1;32m/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD.ipynb Cell 20\u001b[0m in \u001b[0;36mKNN_CF.predict\u001b[0;34m(self, user_id, movie_id)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD.ipynb#X36sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     sim_entities \u001b[39m=\u001b[39m sim_entities[:, mask_rated]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD.ipynb#X36sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m sim_entity_mean \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(sim_entities, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m (np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mclip(sim_entities, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mEPSILON)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD.ipynb#X36sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m sim_r_sum_mean \u001b[39m=\u001b[39m sim_vals \u001b[39m*\u001b[39;49m (sim_entities \u001b[39m-\u001b[39;49m sim_entity_mean[:, \u001b[39mNone\u001b[39;49;00m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD.ipynb#X36sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m prediction \u001b[39m=\u001b[39m entity_mean \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39msum(sim_r_sum_mean) \u001b[39m/\u001b[39m (np\u001b[39m.\u001b[39msum(sim_vals) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mEPSILON)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tnathu-ai/VSCode/recommender-system/SVD/notebook/SVD.ipynb#X36sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m prediction \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(prediction, \u001b[39m0\u001b[39m, \u001b[39m5\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (0,) (0,9724) "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize and train the User based KNN_CF model\n",
    "model_user_based_knn_cf = KNN_CF(k=5, user_based=True)\n",
    "model_user_based_knn_cf.fit(user_item_ratings)\n",
    "\n",
    "# predict and evaluate on the test set\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for _, (user_id, movie_id, rating) in test_df.iterrows():\n",
    "    prediction = model_user_based_knn_cf.predict(user_id, movie_id)\n",
    "    \n",
    "    if prediction is not None:\n",
    "        y_true.append(rating)\n",
    "        y_pred.append(prediction)\n",
    "\n",
    "# assuming that rmse and mae are defined or imported from appropriate library\n",
    "print(f\"Root Mean Squared Error: {rmse(y_true, y_pred)}\")\n",
    "print(f\"Mean Absolute Error: {mae(y_true, y_pred)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------> OBSERVATION: \n",
    "\n",
    "The User-Based Collaborative Filtering model has an RMSE of 1.4821 and an MAE of 1.2579. These values represent the average discrepancy between the predicted and actual ratings. Given the rating scale of 0.5 to 5.0, these results are moderately good, suggesting the model's predictions are reasonably close to the actual ratings. The model took approximately 4 minutes and 16 seconds to run."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item-Based Collaborative Filtering\n",
    "\n",
    "the model finds items that are similar to the target item based on the ratings they received from users. It then predicts the target user's rating for the target item based on the user's ratings for the similar items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 1.422181994356813\n",
      "Mean Absolute Error: 1.1501393384975036\n",
      "CPU times: user 32min 39s, sys: 8min 39s, total: 41min 19s\n",
      "Wall time: 4min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize and train the Item based KNN_CF model\n",
    "model_item_based_knn_cf = KNN_CF(k=5, user_based=False)\n",
    "model_item_based_knn_cf.fit(user_item_ratings)\n",
    "\n",
    "# Predict and evaluate on the test set\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for _, (user_id, movie_id, rating) in test_df.iterrows():\n",
    "    prediction = model_item_based_knn_cf.predict(user_id, movie_id)\n",
    "    \n",
    "    if prediction is not None:\n",
    "        y_true.append(rating)\n",
    "        y_pred.append(prediction)\n",
    "\n",
    "# Assuming that rmse and mae are defined or imported from an appropriate library\n",
    "print(f\"Root Mean Squared Error: {rmse(y_true, y_pred)}\")\n",
    "print(f\"Mean Absolute Error: {mae(y_true, y_pred)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Adaptive Imputation (AutAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def autai_imputation(R, active_user, active_item):\n",
    "    # Initialize the imputed matrix R_prime\n",
    "    R_prime = R.copy()\n",
    "\n",
    "    # Calculate similarity between each pair of users\n",
    "    # not this\n",
    "    sim = cosine_similarity(R)\n",
    "\n",
    "    # Identify the set of items rated by the active user\n",
    "    Ua = np.where(~np.isnan(R[active_user]))[0]\n",
    "\n",
    "    # Identify the set of items co-rated by the active user and each other user\n",
    "    Ts = set()\n",
    "    for ua_prime in Ua:\n",
    "        Taa_prime = np.where(~np.isnan(R[:, ua_prime]))[0]\n",
    "        Ts = Ts.union(Taa_prime)\n",
    "\n",
    "    # Identify the set of ratings by the active user and on the co-rated items\n",
    "    Na_s = R[np.ix_(Ua, list(Ts))]\n",
    "\n",
    "    # Perform imputation for each rating in Na_s\n",
    "    for ua_prime in range(Na_s.shape[0]):\n",
    "        for i in range(Na_s.shape[1]):\n",
    "            if np.isnan(Na_s[ua_prime, i]):\n",
    "                # Calculate the weighted average of the user's ratings, weighted by similarity\n",
    "                weights = sim[ua_prime, :]\n",
    "                ratings = R[:, i]\n",
    "                mask = ~np.isnan(ratings)\n",
    "                r_hat = np.average(ratings[mask], weights=weights[mask])\n",
    "                \n",
    "                # Impute the rating\n",
    "                R_prime[ua_prime, i] = r_hat\n",
    "\n",
    "    return R_prime\n",
    "\n",
    "# Perform imputation on the user-item ratings matrix\n",
    "user_item_ratings_imputed = autai_imputation(user_item_ratings.values, active_user=0, active_item=0)\n",
    "\n",
    "# Convert the imputed ratings matrix to a dataframe\n",
    "user_item_ratings_imputed = pd.DataFrame(user_item_ratings_imputed, index=user_item_ratings.index, columns=user_item_ratings.columns)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_df, test_df = train_test_split(user_item_ratings_imputed, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the User based KNN_CF model\n",
    "model_user_based_knn_cf = KNN_CF(k=5, user_based=False)\n",
    "model_user_based_knn_cf.fit(train_df)\n",
    "\n",
    "# predict and evaluate on the test set\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for _, (user_id, movie_id, rating) in test_df.iterrows():\n",
    "    prediction = model_user_based_knn_cf.predict(user_id, movie_id)\n",
    "    \n",
    "    if prediction is not None:\n",
    "        y_true.append(rating)\n",
    "        y_pred.append(prediction)\n",
    "\n",
    "print(f\"Root Mean Squared Error: {rmse(y_true, y_pred)}\")\n",
    "print(f\"Mean Absolute Error: {mae(y_true, y_pred)}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
