{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model output 1:\n",
      "BLEU-1: 0, BLEU-2: 0, BLEU-3: 0, BLEU-4: 0\n",
      "ROUGE-1: {'r': 0.0, 'p': 0.0, 'f': 0.0}, ROUGE-2: {'r': 0.0, 'p': 0.0, 'f': 0.0}, ROUGE-L: {'r': 0.0, 'p': 0.0, 'f': 0.0}\n",
      "\n",
      "\n",
      "For model output 2:\n",
      "BLEU-1: 0, BLEU-2: 0, BLEU-3: 0, BLEU-4: 0\n",
      "ROUGE-1: {'r': 0.0, 'p': 0.0, 'f': 0.0}, ROUGE-2: {'r': 0.0, 'p': 0.0, 'f': 0.0}, ROUGE-L: {'r': 0.0, 'p': 0.0, 'f': 0.0}\n",
      "\n",
      "\n",
      "For model output 3:\n",
      "BLEU-1: 0, BLEU-2: 0, BLEU-3: 0, BLEU-4: 0\n",
      "ROUGE-1: {'r': 0.0, 'p': 0.0, 'f': 0.0}, ROUGE-2: {'r': 0.0, 'p': 0.0, 'f': 0.0}, ROUGE-L: {'r': 0.0, 'p': 0.0, 'f': 0.0}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "\n",
    "def evaluate_explanations(ground_truth, model_outputs):\n",
    "    # Define a smoothing function\n",
    "    chencherry = SmoothingFunction()\n",
    "    \n",
    "    # Initialize Rouge\n",
    "    rouge = Rouge()\n",
    "    \n",
    "    for i, model_output in enumerate(model_outputs):\n",
    "        # Calculate BLEU scores\n",
    "        if len(ground_truth.split()) < 5:  # Apply smoothing for short sentences\n",
    "            bleu1 = sentence_bleu([ground_truth.split()], model_output.split(), weights=(1, 0, 0, 0), smoothing_function=chencherry.method1)\n",
    "            bleu2 = sentence_bleu([ground_truth.split()], model_output.split(), weights=(0.5, 0.5, 0, 0), smoothing_function=chencherry.method1)\n",
    "            bleu3 = sentence_bleu([ground_truth.split()], model_output.split(), weights=(0.33, 0.33, 0.33, 0), smoothing_function=chencherry.method1)\n",
    "            bleu4 = sentence_bleu([ground_truth.split()], model_output.split(), weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=chencherry.method1)\n",
    "        else:\n",
    "            bleu1 = sentence_bleu([ground_truth.split()], model_output.split(), weights=(1, 0, 0, 0))\n",
    "            bleu2 = sentence_bleu([ground_truth.split()], model_output.split(), weights=(0.5, 0.5, 0, 0))\n",
    "            bleu3 = sentence_bleu([ground_truth.split()], model_output.split(), weights=(0.33, 0.33, 0.33, 0))\n",
    "            bleu4 = sentence_bleu([ground_truth.split()], model_output.split(), weights=(0.25, 0.25, 0.25, 0.25))\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        rouge_scores = rouge.get_scores(hyps=model_output, refs=ground_truth)[0]\n",
    "        \n",
    "        # Print BLEU and ROUGE scores\n",
    "        print(f'For model output {i + 1}:')\n",
    "        print(f'BLEU-1: {bleu1}, BLEU-2: {bleu2}, BLEU-3: {bleu3}, BLEU-4: {bleu4}')\n",
    "        print(f\"ROUGE-1: {rouge_scores['rouge-1']}, ROUGE-2: {rouge_scores['rouge-2']}, ROUGE-L: {rouge_scores['rouge-l']}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Usage:\n",
    "ground_truth = \"Soap\"\n",
    "model_outputs = [\"Great soap\", \"Gentle, effective soap recommended.\", \"The user highly recommends this soap for sensitive skin due to its moisturizing and non-irritating qualities, along with a pleasant, soft fragrance.\"]\n",
    "evaluate_explanations(ground_truth, model_outputs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -----------------> OBSERVATION\n",
    "\n",
    "> In the evaluation metrics presented, both the BLEU and ROUGE scores are 0 for all three model outputs. This suggests that, according to these metrics, none of the model outputs match the reference (or \"ground truth\") text.\n",
    "\n",
    "> BLEU (Bilingual Evaluation Understudy) measures the overlap of n-grams between the generated text and the reference text. In this case, the BLEU score of 0 indicates that there is no overlap of n-grams (for n=1,2,3,4) between your model outputs and the reference text.\n",
    "\n",
    "> ROUGE (Recall-Oriented Understudy for Gisting Evaluation) evaluates the quality of a summary by comparing it to other (typically human-generated) summaries. The 'r', 'p', and 'f' keys in the ROUGE output refer to recall, precision, and F-measure, respectively. All being zero indicates that there were no overlapping n-grams found in the model outputs and the reference summary.\n",
    "\n",
    "> However, one key point to note here is that both BLEU and ROUGE metrics can be less effective or informative when dealing with extremely short texts. These metrics are often used for tasks like machine translation or text summarization where the output texts are generally longer. The 'ground truth' in your case appears to be a single word (\"Soap\"), which might be too short for these metrics to provide a meaningful evaluation.\n",
    "\n",
    "> Therefore, while the scores suggest a complete lack of overlap between the model outputs and the reference, these scores might not be giving a fair or useful evaluation of the quality of the model outputs given the very short length of the reference text.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Advanced_Programming_for_Data_Science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
