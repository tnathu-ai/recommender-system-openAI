{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "from math import sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from math import sqrt\n",
    "import openai\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD from scratch using gradient descent\n",
    "\n",
    "uses the described backpropagation algorithm for gradient descent and singular vectors as described in the [article](https://sifter.org/simon/journal/20070815.html)\n",
    "\n",
    "1. Extract user-item interactions from the ratings dataframe.\n",
    "2. Define the SVD model with functions for initializing the user and movie vectors, predicting ratings, and updating the vectors using gradient descent.\n",
    "3. Train the model on the user-item interactions data.\n",
    "4. Use the learned vectors to make predictions on new user-movie pairs.\n",
    "\n",
    "## Data\n",
    "\n",
    "This dataset (ml-latest-small) describes 5-star rating and free-text tagging activity from [MovieLens](http://movielens.org), a movie recommendation service. It contains 100836 ratings and 3683 tag applications across 9742 movies. These data were created by 610 users between March 29, 1996 and September 24, 2018. This dataset was generated on September 26, 2018.\n",
    "\n",
    "Users were selected at random for inclusion. All selected users had rated at least 20 movies. No demographic information is included. Each user is represented by an id, and no other information is provided.\n",
    "\n",
    "The data are contained in the files `links.csv`, `movies.csv`, `ratings.csv` and `tags.csv`. More details about the contents and use of all these files follows.\n",
    "\n",
    "This is a *development* dataset. As such, it may change over time and is not an appropriate dataset for shared research results. See available *benchmark* datasets if that is your intent.\n",
    "\n",
    "This and other GroupLens data sets are publicly available for download at <http://grouplens.org/datasets/>.\n",
    "\n",
    "\n",
    "## Evaluation metric: \n",
    "+ RMSE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100836 entries, 0 to 100835\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   userId     100836 non-null  int64  \n",
      " 1   movieId    100836 non-null  int64  \n",
      " 2   rating     100836 non-null  float64\n",
      " 3   timestamp  100836 non-null  int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 3.1 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loads the data\n",
    "df = pd.read_csv('../data/ml-latest-small/ratings.csv')\n",
    "df.info()\n",
    "df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### --------> OBSERVATIONS:\n",
    "\n",
    "+ movieId: A unique identifier for the movie.\n",
    "+ title: The title of the movie, along with its release year in parentheses.\n",
    "+ genres: The genres associated with the movie, separated by pipe characters (|)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 610\n",
      "\n",
      "Number of unique movies: 9724\n",
      "\n",
      "Number of unique ratings: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unique users\n",
    "print(f'Number of unique users: {df.userId.unique().shape[0]}\\n')\n",
    "\n",
    "# unique movies\n",
    "print(f'Number of unique movies: {df.movieId.unique().shape[0]}\\n')\n",
    "\n",
    "# unique ratings\n",
    "print(f'Number of unique ratings: {df.rating.unique().shape[0]}\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Any infinities in the data with NaN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values and infinities\n",
    "df.isnull().sum()\n",
    "df.isnull().values.any()\n",
    "# check for infinities\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splits the data into a training set and a test set using a user-stratified train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M50 set:\n",
      "        userId  movieId  rating   timestamp\n",
      "2977       20        2     3.0  1054038313\n",
      "2978       20        8     1.0  1054038422\n",
      "2979       20       13     4.0  1054038425\n",
      "2980       20       34     4.0  1054038093\n",
      "2981       20       48     5.0  1054038357\n",
      "...       ...      ...     ...         ...\n",
      "95960     601   170705     5.0  1521397596\n",
      "95961     601   172591     4.5  1521467819\n",
      "95962     601   174055     4.0  1521397739\n",
      "95963     601   176371     4.0  1521397623\n",
      "95964     601   177765     4.5  1521397621\n",
      "\n",
      "[8719 rows x 4 columns]\n",
      "M100 set:\n",
      "        userId  movieId  rating   timestamp\n",
      "1569       16       47     3.5  1377477814\n",
      "1570       16       50     4.0  1377476781\n",
      "1571       16      111     4.5  1377477446\n",
      "1572       16      204     2.0  1377476617\n",
      "1573       16      260     3.0  1377476936\n",
      "...       ...      ...     ...         ...\n",
      "99529     609      892     3.0   847221080\n",
      "99530     609     1056     3.0   847221080\n",
      "99531     609     1059     3.0   847221054\n",
      "99532     609     1150     4.0   847221054\n",
      "99533     609     1161     4.0   847221080\n",
      "\n",
      "[22816 rows x 4 columns]\n",
      "M400 set:\n",
      "         userId  movieId  rating   timestamp\n",
      "0            1        1     4.0   964982703\n",
      "1            1        3     4.0   964981247\n",
      "2            1        6     4.0   964982224\n",
      "3            1       47     5.0   964983815\n",
      "4            1       50     5.0   964982931\n",
      "...        ...      ...     ...         ...\n",
      "100831     610   166534     4.0  1493848402\n",
      "100832     610   168248     5.0  1493850091\n",
      "100833     610   168250     5.0  1494273047\n",
      "100834     610   168252     5.0  1493846352\n",
      "100835     610   170875     3.0  1493846415\n",
      "\n",
      "[56119 rows x 4 columns]\n",
      "Test set:\n",
      "        userId  movieId  rating   timestamp\n",
      "1355       13       47     5.0   987895819\n",
      "1356       13      305     1.0   987456968\n",
      "1357       13      597     3.0   987456968\n",
      "1358       13     1173     3.0   987457052\n",
      "1359       13     1198     5.0   987456848\n",
      "...       ...      ...     ...         ...\n",
      "99492     608    51903     2.5  1189477736\n",
      "99493     608    51935     4.0  1189563886\n",
      "99494     608    52245     3.0  1189563917\n",
      "99495     608    53996     5.0  1189380659\n",
      "99496     608    54503     4.5  1189368725\n",
      "\n",
      "[13182 rows x 4 columns]\n",
      "Training set:\n",
      "         userId  movieId  rating   timestamp\n",
      "21452      140     4234     3.0  1012505945\n",
      "22899      156     2080     1.0   951113118\n",
      "58090      380   182639     4.0  1536874706\n",
      "79604      495     5254     3.5  1458636268\n",
      "100382     610    69134     3.0  1493848172\n",
      "...        ...      ...     ...         ...\n",
      "49818      318   136024     3.0  1455886225\n",
      "66934      432     5507     1.5  1315242710\n",
      "74191      474     4012     2.0  1046887657\n",
      "80857      510      497     1.5  1141158809\n",
      "40375      274    51709     3.5  1285897528\n",
      "\n",
      "[80668 rows x 4 columns]\n",
      "Test set:\n",
      "       userId  movieId  rating   timestamp\n",
      "0          1     1920     4.0   964981780\n",
      "1          1      457     5.0   964981909\n",
      "2          1     2648     4.0   964983414\n",
      "3          1      316     3.0   964982310\n",
      "4          1      661     5.0   964982838\n",
      "...      ...      ...     ...         ...\n",
      "5247     610     4020     3.5  1479542683\n",
      "5248     610    90600     3.5  1493847740\n",
      "5249     610    57669     5.0  1493845166\n",
      "5250     610     6287     3.0  1493847091\n",
      "5251     610     6620     4.0  1493845340\n",
      "\n",
      "[5252 rows x 4 columns]\n",
      "All-But-One Training set:\n",
      "         userId  movieId  rating   timestamp\n",
      "0            1        1     4.0   964982703\n",
      "1            1        3     4.0   964981247\n",
      "2            1        6     4.0   964982224\n",
      "3            1       47     5.0   964983815\n",
      "4            1       50     5.0   964982931\n",
      "...        ...      ...     ...         ...\n",
      "100831     610   166534     4.0  1493848402\n",
      "100832     610   168248     5.0  1493850091\n",
      "100833     610   168250     5.0  1494273047\n",
      "100834     610   168252     5.0  1493846352\n",
      "100835     610   170875     3.0  1493846415\n",
      "\n",
      "[100226 rows x 4 columns]\n",
      "All-But-One Test set:\n",
      "        userId  movieId  rating   timestamp\n",
      "219         1     3578     5.0   964980668\n",
      "245         2    77455     3.0  1445714941\n",
      "294         3     6835     5.0  1306463670\n",
      "306         4      106     4.0   986848784\n",
      "548         5      527     5.0   847434960\n",
      "...       ...      ...     ...         ...\n",
      "98227     606     7131     3.0  1171813499\n",
      "98506     607      423     3.0   963080410\n",
      "98707     608      188     3.5  1117503407\n",
      "99501     609      137     3.0   847221054\n",
      "99587     610      903     5.0  1479542931\n",
      "\n",
      "[610 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "def split_data_by_rated_items(df, test_size, given_n):\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=42, stratify=df['userId'])\n",
    "\n",
    "    # For each user in the test set, keep only 'given_n' rated items if they have rated that many,\n",
    "    # otherwise keep all the items they have rated.\n",
    "    test_df = test_df.groupby('userId').apply(lambda x: x.sample(min(len(x), given_n), random_state=42))\n",
    "\n",
    "    return train_df, test_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def split_data_by_unique_users(df):\n",
    "    unique_users = df['userId'].unique()\n",
    "    np.random.shuffle(unique_users)\n",
    "\n",
    "    # Get the user IDs for each set\n",
    "    M50_users = unique_users[:50]\n",
    "    M100_users = unique_users[50:150]\n",
    "    M400_users = unique_users[150:550]\n",
    "    test_users = unique_users[550:]\n",
    "\n",
    "    # Split the DataFrame into the different sets based on the user IDs\n",
    "    M50_df = df[df['userId'].isin(M50_users)]\n",
    "    M100_df = df[df['userId'].isin(M100_users)]\n",
    "    M400_df = df[df['userId'].isin(M400_users)]\n",
    "    test_df = df[df['userId'].isin(test_users)]\n",
    "\n",
    "    return M50_df, M100_df, M400_df, test_df\n",
    "\n",
    "\n",
    "def all_but_one(df):\n",
    "    # For each user, select one rating and split it into a separate DataFrame\n",
    "    test_df = df.groupby('userId').sample(n=1, random_state=42)\n",
    "    train_df = df.drop(test_df.index)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# Call the function\n",
    "M50_df, M100_df, M400_df, test_df = split_data_by_unique_users(df)\n",
    "\n",
    "print('M50 set:\\n', M50_df)\n",
    "print('M100 set:\\n', M100_df)\n",
    "print('M400 set:\\n', M400_df)\n",
    "print('Test set:\\n', test_df)\n",
    "\n",
    "# Call the functions\n",
    "train_df_given_10, test_df_given_10 = split_data_by_rated_items(df, test_size=0.2, given_n=10)  # Modify test_size and given_n as needed\n",
    "print('Training set:\\n', train_df_given_10)\n",
    "print('Test set:\\n', test_df_given_10)\n",
    "\n",
    "train_df, test_df = all_but_one(df)\n",
    "print('All-But-One Training set:\\n', train_df)\n",
    "print('All-But-One Test set:\\n', test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "+ RMSE\n",
    "+ MAE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE (Root Mean Squared Error) from scratch\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"Compute the Root Mean Squared Error (RMSE).\n",
    "    \n",
    "    Args:\n",
    "        y_true (list): Actual values.\n",
    "        y_pred (list): Predicted values.\n",
    "\n",
    "    Returns:\n",
    "        float: RMSE value.\n",
    "    \"\"\"\n",
    "    error = 0\n",
    "    # Calculate squared difference for each pair of actual and predicted values\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        error += (true - pred) ** 2\n",
    "    # Average out the squared differences\n",
    "    error /= len(y_true)\n",
    "    # Return the square root of the averaged squared differences\n",
    "    return sqrt(error)\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    \"\"\"Compute the Mean Absolute Error (MAE).\n",
    "    \n",
    "    Args:\n",
    "        y_true (list): Actual values.\n",
    "        y_pred (list): Predicted values.\n",
    "\n",
    "    Returns:\n",
    "        float: MAE value.\n",
    "    \"\"\"\n",
    "    # Calculate absolute difference for each pair and average them\n",
    "    return np.mean(np.abs(np.array(y_true) - np.array(y_pred)))\n",
    "\n",
    "def f1_score(precisions, recalls):\n",
    "    \"\"\"Compute the F1 score for each user based on provided precision and recall values.\n",
    "    \n",
    "    Args:\n",
    "        precisions (dict): Precision values keyed by user ID.\n",
    "        recalls (dict): Recall values keyed by user ID.\n",
    "\n",
    "    Returns:\n",
    "        dict: F1 scores keyed by user ID.\n",
    "    \"\"\"\n",
    "    f1_scores = dict()\n",
    "    # Calculate F1 score for each user\n",
    "    for uid in precisions.keys():\n",
    "        p, r = precisions[uid], recalls[uid]\n",
    "        f1_scores[uid] = 2*(p*r) / (p + r) if (p + r) != 0 else 0\n",
    "    return f1_scores\n",
    "\n",
    "def precision_at_k(predictions, k=10, threshold=3.5):\n",
    "    \"\"\"Compute the precision at top k predictions for each user.\n",
    "    \n",
    "    Args:\n",
    "        predictions (list): List of tuples containing user ID, unknown data, actual rating, and predicted rating.\n",
    "        k (int, optional): Number of top predictions to consider. Defaults to 10.\n",
    "        threshold (float, optional): Minimum rating to consider a prediction as relevant. Defaults to 3.5.\n",
    "\n",
    "    Returns:\n",
    "        dict: Precision at k values keyed by user ID.\n",
    "    \"\"\"\n",
    "    user_est_true = defaultdict(list)\n",
    "    # Organize predictions by user ID\n",
    "    for uid, _, true_r, est in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        # Count predictions above threshold in top k\n",
    "        n_rel_and_rec_k = sum((true_r >= threshold) for (_, true_r) in user_ratings[:k])\n",
    "        precisions[uid] = n_rel_and_rec_k / k\n",
    "\n",
    "    return precisions\n",
    "\n",
    "def recall_at_k(predictions, k=10, threshold=3.5):\n",
    "    \"\"\"Compute the recall at top k predictions for each user.\n",
    "    \n",
    "    Args:\n",
    "        predictions (list): List of tuples containing user ID, unknown data, actual rating, and predicted rating.\n",
    "        k (int, optional): Number of top predictions to consider. Defaults to 10.\n",
    "        threshold (float, optional): Minimum rating to consider a prediction as relevant. Defaults to 3.5.\n",
    "\n",
    "    Returns:\n",
    "        dict: Recall at k values keyed by user ID.\n",
    "    \"\"\"\n",
    "    user_est_true = defaultdict(list)\n",
    "    # Organize predictions by user ID\n",
    "    for uid, _, true_r, est in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        # Count actual relevant items and those in top k\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "        n_rel_and_rec_k = sum((true_r >= threshold) for (_, true_r) in user_ratings[:k])\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "\n",
    "    return recalls\n",
    "\n",
    "def ndcg_at_k(predictions, k=10):\n",
    "    \"\"\"Compute the Normalized Discounted Cumulative Gain at k.\n",
    "    \n",
    "    Args:\n",
    "        predictions (list): List of tuples containing user ID, unknown data, actual rating, and predicted rating.\n",
    "        k (int, optional): Number of top predictions to consider. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        float: Average NDCG value across users.\n",
    "    \"\"\"\n",
    "    user_est_true = defaultdict(list)\n",
    "    # Organize predictions by user ID\n",
    "    for uid, _, true_r, est in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    ndcg_values = []\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        # Compute DCG and IDCG for user and then compute NDCG\n",
    "        dcg = sum((rel / np.log2(ind + 2)) for ind, (est, rel) in enumerate(user_ratings[:k]))\n",
    "        idcg = sum((rel / np.log2(ind + 2)) for ind, (est, rel) in enumerate(sorted(user_ratings, key=lambda x: x[1], reverse=True)[:k]))\n",
    "        ndcg_values.append(dcg / idcg if idcg > 0.0 else 0.0)\n",
    "\n",
    "    return np.mean(ndcg_values)\n",
    "\n",
    "def evaluate(predictions, k=10, threshold=3.5):\n",
    "    \"\"\"Evaluate predictions on various metrics: RMSE, MAE, Precision@k, Recall@k, and NDCG.\n",
    "    \n",
    "    Args:\n",
    "        predictions (list): List of tuples containing user ID, unknown data, actual rating, and predicted rating.\n",
    "        k (int, optional): Number of top predictions to consider for Precision@k, Recall@k, and NDCG. Defaults to 10.\n",
    "        threshold (float, optional): Minimum rating to consider a prediction as relevant for Precision@k and Recall@k. Defaults to 3.5.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing RMSE, MAE, Precision@k, Recall@k, and NDCG values.\n",
    "    \"\"\"\n",
    "    # Calculate each metric and return them in a dictionary\n",
    "    precisions = precision_at_k(predictions, k=k, threshold=threshold)\n",
    "    recalls = recall_at_k(predictions, k=k, threshold=threshold)\n",
    "    ndcg = ndcg_at_k(predictions, k=k)\n",
    "    \n",
    "    return {\n",
    "        'RMSE': rmse([true_r for uid, _, true_r, _ in predictions], [est for _, _, true_r, est in predictions]),\n",
    "        'MAE': mae([true_r for uid, _, true_r, _ in predictions], [est for _, _, true_r, est in predictions]),\n",
    "        'Precision@k': sum(prec for prec in precisions.values()) / len(precisions),\n",
    "        'Recall@k': sum(rec for rec in recalls.values()) / len(recalls),\n",
    "        'NDCG': ndcg\n",
    "    }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD\n",
    "\n",
    "+ \"cold-start handling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD:\n",
    "    def __init__(self, num_factors, learning_rate, num_epochs, top_n=10):\n",
    "        # Initializing the instance variables with given arguments\n",
    "        self.num_factors = num_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.top_n = top_n  # number of movies to recommend for cold start\n",
    "\n",
    "    def fit(self, user_item_ratings):\n",
    "        # Initializing the user and movie latent factors matrices with random numbers\n",
    "        self.user_factors = np.random.randn(user_item_ratings.userId.nunique(), self.num_factors)\n",
    "        self.movie_factors = np.random.randn(user_item_ratings.movieId.nunique(), self.num_factors)\n",
    "        \n",
    "        # Creating dictionaries to map user and movie IDs to their respective indices in the factor matrices\n",
    "        self.user_index = {user_id: idx for idx, user_id in enumerate(user_item_ratings.userId.unique())}\n",
    "        self.movie_index = {movie_id: idx for idx, movie_id in enumerate(user_item_ratings.movieId.unique())}\n",
    "\n",
    "        # Calculate average rating for each movie\n",
    "        self.movie_avg_rating = user_item_ratings.groupby('movieId')['rating'].mean().to_dict()\n",
    "\n",
    "        # Get top-N movies based on average rating for cold start problem\n",
    "        sorted_movies_by_avg_rating = sorted(self.movie_avg_rating.items(), key=lambda x: x[1], reverse=True)\n",
    "        self.top_n_movies = [movie_id for movie_id, _ in sorted_movies_by_avg_rating[:self.top_n]]\n",
    "\n",
    "        # Loop over epochs\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # Loop over all user-item-rating rows in the DataFrame\n",
    "            for idx, row in user_item_ratings.iterrows():\n",
    "                user_id = row['userId']\n",
    "                movie_id = row['movieId']\n",
    "                rating = row['rating']\n",
    "\n",
    "                # Getting the user and movie indices for the current user-item pair\n",
    "                user_idx = self.user_index[user_id]\n",
    "                movie_idx = self.movie_index[movie_id]\n",
    "\n",
    "                # Computing the predicted rating as the dot product of the user and movie factors\n",
    "                prediction = np.dot(self.user_factors[user_idx], self.movie_factors[movie_idx])\n",
    "                # Computing the error as the difference between the actual and predicted ratings\n",
    "                error = rating - prediction\n",
    "\n",
    "                # Updating the user and movie factor vectors in the direction that minimizes the error\n",
    "                self.user_factors[user_idx] += self.learning_rate * error * self.movie_factors[movie_idx]\n",
    "                self.movie_factors[movie_idx] += self.learning_rate * error * self.user_factors[user_idx]\n",
    "\n",
    "    def predict(self, user_id, movie_id):\n",
    "        # Getting the user and movie indices for the given user-item pair\n",
    "        user_idx = self.user_index.get(user_id, -1)\n",
    "        movie_idx = self.movie_index.get(movie_id, -1)\n",
    "\n",
    "        # If the user or the movie is not present in the training data, return the movie's average rating\n",
    "        if user_idx == -1 or movie_idx == -1:\n",
    "            return self.movie_avg_rating.get(movie_id)\n",
    "\n",
    "        # Otherwise, return the predicted rating as the dot product of the user and movie factors\n",
    "        return np.dot(self.user_factors[user_idx], self.movie_factors[movie_idx])\n",
    "\n",
    "    def recommend(self, user_id):\n",
    "        # If the user is not present in the training data, return top-N movies\n",
    "        if user_id not in self.user_index:\n",
    "            return self.top_n_movies\n",
    "\n",
    "        # Otherwise, predict the rating for each movie and return the top-N movies\n",
    "        user_ratings = {movie_id: self.predict(user_id, movie_id) for movie_id in self.movie_index.keys()}\n",
    "        sorted_user_ratings = sorted(user_ratings.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [movie_id for movie_id, _ in sorted_user_ratings[:self.top_n]]\n",
    "    \n",
    "\n",
    "svd = SVD(num_factors=35, learning_rate=0.01, num_epochs=10, top_n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(df, model):\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    # Fit the model to the data\n",
    "    model.fit(df)\n",
    "\n",
    "    # Predict ratings for the Test set and evaluate\n",
    "    test_predictions = test_df.apply(lambda row: model.predict(row['userId'], row['movieId']), axis=1)\n",
    "    \n",
    "    # Remove None values and corresponding actual ratings\n",
    "    actual_ratings = test_df['rating'][test_predictions.notna()]\n",
    "    test_predictions = test_predictions.dropna()\n",
    "\n",
    "    svd_predictions = [(uid, iid, true_r, est) for uid, iid, true_r, est in zip(test_df['userId'], test_df['movieId'], actual_ratings, test_predictions)]\n",
    "    \n",
    "    # Compute metrics for the model\n",
    "    metrics = evaluate(svd_predictions)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Execution time for: {end_time - start_time} seconds\")\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for M50 dataset\n",
      "Execution time for: 3.5147593021392822 seconds\n",
      "The evaluation metrics for the SVD model are: {'RMSE': 1.0772998397808686, 'MAE': 0.8250792022416213, 'Precision@k': 0.06698292220113895, 'Recall@k': 1.0, 'NDCG': 1.0}\n",
      "\n",
      "Execution time for M100 dataset\n",
      "Execution time for: 9.22215223312378 seconds\n",
      "The evaluation metrics for the SVD model are: {'RMSE': 0.9895066834662088, 'MAE': 0.7521875499473722, 'Precision@k': 0.06689655172413841, 'Recall@k': 1.0, 'NDCG': 1.0}\n",
      "\n",
      "Execution time for M400 dataset\n",
      "Execution time for: 22.76992893218994 seconds\n",
      "The evaluation metrics for the SVD model are: {'RMSE': 0.8220405753392208, 'MAE': 0.5768161861180655, 'Precision@k': 0.06639072847682169, 'Recall@k': 1.0, 'NDCG': 1.0}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Precision@k</th>\n",
       "      <th>Recall@k</th>\n",
       "      <th>NDCG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M50</td>\n",
       "      <td>1.077300</td>\n",
       "      <td>0.825079</td>\n",
       "      <td>0.066983</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M100</td>\n",
       "      <td>0.989507</td>\n",
       "      <td>0.752188</td>\n",
       "      <td>0.066897</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M400</td>\n",
       "      <td>0.822041</td>\n",
       "      <td>0.576816</td>\n",
       "      <td>0.066391</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset      RMSE       MAE  Precision@k  Recall@k  NDCG\n",
       "0     M50  1.077300  0.825079     0.066983       1.0   1.0\n",
       "1    M100  0.989507  0.752188     0.066897       1.0   1.0\n",
       "2    M400  0.822041  0.576816     0.066391       1.0   1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Execution time for M50 dataset\")\n",
    "svd_metrics_M50 = evaluate_model(M50_df, svd)\n",
    "print(f\"The evaluation metrics for the SVD model are: {svd_metrics_M50}\\n\")\n",
    "\n",
    "print(f\"Execution time for M100 dataset\")\n",
    "svd_metrics_M100 = evaluate_model(M100_df, svd)\n",
    "print(f\"The evaluation metrics for the SVD model are: {svd_metrics_M100}\\n\")\n",
    "\n",
    "print(f\"Execution time for M400 dataset\")\n",
    "svd_metrics_M400 = evaluate_model(M400_df, svd)\n",
    "print(f\"The evaluation metrics for the SVD model are: {svd_metrics_M400}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Convert the metrics to DataFrame\n",
    "df_M50 = pd.DataFrame([svd_metrics_M50])\n",
    "df_M50['Dataset'] = 'M50'\n",
    "\n",
    "df_M100 = pd.DataFrame([svd_metrics_M100])\n",
    "df_M100['Dataset'] = 'M100'\n",
    "\n",
    "df_M400 = pd.DataFrame([svd_metrics_M400])\n",
    "df_M400['Dataset'] = 'M400'\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "metrics_df = pd.concat([df_M50, df_M100, df_M400], ignore_index=True)\n",
    "\n",
    "# Reorder the columns\n",
    "cols = metrics_df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]  # Move the last column to first\n",
    "metrics_df = metrics_df[cols]\n",
    "\n",
    "metrics_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN based CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_CF:\n",
    "    def __init__(self, n_users, n_items, k=3, gamma=0, delta=25, epsilon=1e-9):\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.k = k\n",
    "        self.gamma = gamma\n",
    "        self.delta = delta\n",
    "        self.epsilon = epsilon\n",
    "        self.user_corrs = np.zeros((n_users, n_users))\n",
    "        self.item_corrs = np.zeros((n_items, n_items))\n",
    "\n",
    "    def fit(self, user_item_matrix):\n",
    "        # user-based\n",
    "        for i in range(self.n_users):\n",
    "            for j in range(self.n_users):\n",
    "                self.user_corrs[i, j] = self.pearson_corr(user_item_matrix[i], user_item_matrix[j])\n",
    "\n",
    "        # item-based\n",
    "        for i in range(self.n_items):\n",
    "            for j in range(self.n_items):\n",
    "                self.item_corrs[i, j] = self.pearson_corr(user_item_matrix[:, i], user_item_matrix[:, j])\n",
    "\n",
    "    def predict(self, user_item_matrix, mode='user'):\n",
    "        predictions = np.zeros((self.n_users, self.n_items))\n",
    "        if mode == 'user':\n",
    "            for i in range(self.n_users):\n",
    "                for j in range(self.n_items):\n",
    "                    if user_item_matrix[i, j] > 0:\n",
    "                        sim_users = np.argsort(self.user_corrs[i])[-(self.k + 1):-1]\n",
    "                        predictions[i, j] = self.predict_rating(user_item_matrix, sim_users, i, j, mode)\n",
    "        elif mode == 'item':\n",
    "            for i in range(self.n_users):\n",
    "                for j in range(self.n_items):\n",
    "                    if user_item_matrix[i, j] > 0:\n",
    "                        sim_items = np.argsort(self.item_corrs[j])[-(self.k + 1):-1]\n",
    "                        predictions[i, j] = self.predict_rating(user_item_matrix, sim_items, i, j, mode)\n",
    "        return predictions\n",
    "\n",
    "    def pearson_corr(self, vec_i, vec_j):\n",
    "        mask_i = vec_i > 0\n",
    "        mask_j = vec_j > 0\n",
    "        corrated_index = np.intersect1d(np.where(mask_i), np.where(mask_j))\n",
    "        if len(corrated_index) == 0:\n",
    "            return 0\n",
    "        mean_i = np.mean(vec_i[corrated_index])\n",
    "        mean_j = np.mean(vec_j[corrated_index])\n",
    "        sub_i = vec_i[corrated_index] - mean_i\n",
    "        sub_j = vec_j[corrated_index] - mean_j\n",
    "        return np.sum(sub_i * sub_j) / (np.sqrt(np.sum(np.square(sub_i))) * np.sqrt(np.sum(np.square(sub_j))) + self.epsilon)\n",
    "\n",
    "    def predict_rating(self, user_item_matrix, sim_indices, i, j, mode):\n",
    "        if mode == 'user':\n",
    "            sim_ratings = user_item_matrix[sim_indices, j]\n",
    "            sim_means = np.array([np.mean(user_item_matrix[k][user_item_matrix[k]>0]) for k in sim_indices])\n",
    "            sim_vals = self.user_corrs[i][sim_indices]\n",
    "        elif mode == 'item':\n",
    "            sim_ratings = user_item_matrix[i, sim_indices]\n",
    "            sim_means = np.array([np.mean(user_item_matrix[:, k][user_item_matrix[:, k]>0]) for k in sim_indices])\n",
    "            sim_vals = self.item_corrs[j][sim_indices]\n",
    "        if np.sum(sim_vals) == 0:\n",
    "            return np.mean(sim_ratings)\n",
    "        else:\n",
    "            return np.mean(sim_ratings) + np.sum(sim_vals * (sim_ratings - sim_means)) / np.sum(sim_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_matrix(df, nrows, ncols):\n",
    "    matrix = np.zeros((nrows, ncols))\n",
    "    for row in df.itertuples():\n",
    "        matrix[row.userId, row.movieId] = row.rating\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23min 19s, sys: 7.33 s, total: 23min 26s\n",
      "Wall time: 33min 39s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Precision@k</th>\n",
       "      <th>Recall@k</th>\n",
       "      <th>NDCG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M100</td>\n",
       "      <td>7.722212</td>\n",
       "      <td>7.631977</td>\n",
       "      <td>0.069</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset      RMSE       MAE  Precision@k  Recall@k  NDCG\n",
       "0    M100  7.722212  7.631977        0.069       1.0   1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create mappings for userIds and movieIds to contiguous indices\n",
    "user_mapping = {user_id: i for i, user_id in enumerate(M100_df['userId'].unique())}\n",
    "movie_mapping = {movie_id: i for i, movie_id in enumerate(M100_df['movieId'].unique())}\n",
    "\n",
    "# Create reverse mappings for later use\n",
    "reverse_user_mapping = {i: user_id for user_id, i in user_mapping.items()}\n",
    "reverse_movie_mapping = {i: movie_id for movie_id, i in movie_mapping.items()}\n",
    "\n",
    "# Apply the mappings to the dataframes\n",
    "M100_df['userId'] = M100_df['userId'].map(user_mapping)\n",
    "M100_df['movieId'] = M100_df['movieId'].map(movie_mapping)\n",
    "\n",
    "test_df['userId'] = test_df['userId'].map(user_mapping)\n",
    "test_df['movieId'] = test_df['movieId'].map(movie_mapping)\n",
    "\n",
    "# Drop rows with NaN userId or movieId\n",
    "test_df.dropna(subset=['userId', 'movieId'], inplace=True)\n",
    "\n",
    "# Convert userId and movieId to integer\n",
    "test_df['userId'] = test_df['userId'].astype(int)\n",
    "test_df['movieId'] = test_df['movieId'].astype(int)\n",
    "\n",
    "\n",
    "n_users = M100_df['userId'].nunique()\n",
    "n_items = M100_df['movieId'].nunique()\n",
    "\n",
    "train_matrix = df_to_matrix(M100_df, n_users, n_items)\n",
    "test_matrix = df_to_matrix(test_df, n_users, n_items)\n",
    "\n",
    "knn_cf = KNN_CF(n_users, n_items, k=3)\n",
    "\n",
    "# Fit the model to the M100 data\n",
    "knn_cf.fit(train_matrix)\n",
    "\n",
    "# Predict ratings for the Test set and evaluate\n",
    "user_based_predictions = knn_cf.predict(test_matrix, mode='user')\n",
    "test_predictions = user_based_predictions[test_matrix.nonzero()]\n",
    "actual_ratings = test_matrix[test_matrix.nonzero()]\n",
    "\n",
    "knn_predictions = [(uid, iid, true_r, est) for uid, iid, true_r, est in zip(test_df['userId'], test_df['movieId'], actual_ratings, test_predictions)]\n",
    "# Compute metrics for the KNN model\n",
    "knn_metrics_M100 = evaluate(knn_predictions)\n",
    "\n",
    "# create a dataframe for the results \n",
    "knn_results = pd.DataFrame(knn_metrics_M100, index=[0])\n",
    "# add first column of the dataframe as the dataset name\n",
    "knn_results.insert(0, 'dataset', 'M100')\n",
    "knn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 19s, sys: 2.95 s, total: 8min 22s\n",
      "Wall time: 8min 24s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Precision@k</th>\n",
       "      <th>Recall@k</th>\n",
       "      <th>NDCG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset  RMSE  MAE  Precision@k  Recall@k  NDCG\n",
       "0     M50   NaN  NaN          0.1       1.0   1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Create mappings for userIds and movieIds to contiguous indices\n",
    "user_mapping = {user_id: i for i, user_id in enumerate(M50_df['userId'].unique())}\n",
    "movie_mapping = {movie_id: i for i, movie_id in enumerate(M50_df['movieId'].unique())}\n",
    "\n",
    "# Create reverse mappings for later use\n",
    "reverse_user_mapping = {i: user_id for user_id, i in user_mapping.items()}\n",
    "reverse_movie_mapping = {i: movie_id for movie_id, i in movie_mapping.items()}\n",
    "\n",
    "# Apply the mappings to the dataframes\n",
    "M50_df['userId'] = M50_df['userId'].map(user_mapping)\n",
    "M50_df['movieId'] = M50_df['movieId'].map(movie_mapping)\n",
    "\n",
    "test_df['userId'] = test_df['userId'].map(user_mapping)\n",
    "test_df['movieId'] = test_df['movieId'].map(movie_mapping)\n",
    "\n",
    "# Drop rows with NaN userId or movieId\n",
    "test_df.dropna(subset=['userId', 'movieId'], inplace=True)\n",
    "\n",
    "# Convert userId and movieId to integer\n",
    "test_df['userId'] = test_df['userId'].astype(int)\n",
    "test_df['movieId'] = test_df['movieId'].astype(int)\n",
    "\n",
    "\n",
    "n_users = M50_df['userId'].nunique()\n",
    "n_items = M50_df['movieId'].nunique()\n",
    "\n",
    "train_matrix = df_to_matrix(M50_df, n_users, n_items)\n",
    "test_matrix = df_to_matrix(test_df, n_users, n_items)\n",
    "\n",
    "knn_cf = KNN_CF(n_users, n_items, k=3)\n",
    "\n",
    "# Fit the model to the M100 data\n",
    "knn_cf.fit(train_matrix)\n",
    "\n",
    "# Predict ratings for the Test set and evaluate\n",
    "user_based_predictions = knn_cf.predict(test_matrix, mode='user')\n",
    "test_predictions = user_based_predictions[test_matrix.nonzero()]\n",
    "actual_ratings = test_matrix[test_matrix.nonzero()]\n",
    "\n",
    "knn_predictions = [(uid, iid, true_r, est) for uid, iid, true_r, est in zip(test_df['userId'], test_df['movieId'], actual_ratings, test_predictions)]\n",
    "# Compute metrics for the KNN model\n",
    "knn_results_M50 = evaluate(knn_predictions)\n",
    "\n",
    "# create a dataframe to concatenate the results\n",
    "knn_results_M50 = pd.DataFrame(knn_results_M50, index=[0])\n",
    "# add first column of the dataframe as the dataset name\n",
    "knn_results_M50.insert(0, 'dataset', 'M50')\n",
    "knn_results_M50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
