{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current directory: /Users/tnathu-ai/VSCode/recommender-system/recommender-system-openAI/rec-sys/data/amazon-beauty\n",
      "embedding model current directory: /Users/tnathu-ai/VSCode/recommender-system/recommender-system-openAI/rec-sys/models/embedding\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import openai\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import (\n",
    "    get_embedding,\n",
    "    distances_from_embeddings,\n",
    "    tsne_components_from_embeddings,\n",
    "    chart_from_components,\n",
    "    indices_of_nearest_neighbors_from_distances,\n",
    ")\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Add the path to the constants file to the system path\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from constants import (\n",
    "    RANDOM_STATE, \n",
    "    OPENAI_API_KEY\n",
    ")\n",
    "# OpenAI API Key\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "# constants\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "# Get the current directory of the notebook\n",
    "current_dir = os.path.dirname(os.path.abspath(\"../../data/amazon-beauty/parse_and_clean_meta_data.ipynb\"))\n",
    "# # Get the current directory of the notebook\n",
    "embedding_model_current_dir = os.path.dirname(os.path.abspath(\"../../models/embedding/parse_and_clean_meta_data.ipynb\"))\n",
    "print(f\"current directory: {current_dir}\")\n",
    "print(f\"embedding model current directory: {embedding_model_current_dir}\")\n",
    "\n",
    "n_examples = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data path: /Users/tnathu-ai/VSCode/recommender-system/recommender-system-openAI/rec-sys/data/amazon-beauty/merged_data.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>ANV9L0JU6BNL</td>\n",
       "      <td>B000052YAN</td>\n",
       "      <td>best floss i've used. does not break as easily...</td>\n",
       "      <td>best floss i've used</td>\n",
       "      <td>Reach Dentotape Waxed Dental Floss with Extra ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>ANV9L0JU6BNL</td>\n",
       "      <td>B000052YAN</td>\n",
       "      <td>best floss i've used. does not break as easily...</td>\n",
       "      <td>best floss i've used</td>\n",
       "      <td>Reach Dentotape Waxed Dental Floss with Extra ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>A2TU781PWGS09X</td>\n",
       "      <td>B00006L9LC</td>\n",
       "      <td>Doesnt smell</td>\n",
       "      <td>Two Stars</td>\n",
       "      <td>Citre Shine Moisture Burst Shampoo - 16 fl oz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>A2TU781PWGS09X</td>\n",
       "      <td>B00006L9LC</td>\n",
       "      <td>Doesnt smell</td>\n",
       "      <td>Two Stars</td>\n",
       "      <td>Citre Shine Moisture Burst Shampoo - 16 fl oz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>A3A8F2URN7MEPR</td>\n",
       "      <td>B00006L9LC</td>\n",
       "      <td>My favorite powder!</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Citre Shine Moisture Burst Shampoo - 16 fl oz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating      reviewerID        asin  \\\n",
       "0     5.0    ANV9L0JU6BNL  B000052YAN   \n",
       "1     5.0    ANV9L0JU6BNL  B000052YAN   \n",
       "2     2.0  A2TU781PWGS09X  B00006L9LC   \n",
       "3     2.0  A2TU781PWGS09X  B00006L9LC   \n",
       "4     5.0  A3A8F2URN7MEPR  B00006L9LC   \n",
       "\n",
       "                                          reviewText               summary  \\\n",
       "0  best floss i've used. does not break as easily...  best floss i've used   \n",
       "1  best floss i've used. does not break as easily...  best floss i've used   \n",
       "2                                       Doesnt smell             Two Stars   \n",
       "3                                       Doesnt smell             Two Stars   \n",
       "4                                My favorite powder!            Five Stars   \n",
       "\n",
       "                                               title  \n",
       "0  Reach Dentotape Waxed Dental Floss with Extra ...  \n",
       "1  Reach Dentotape Waxed Dental Floss with Extra ...  \n",
       "2      Citre Shine Moisture Burst Shampoo - 16 fl oz  \n",
       "3      Citre Shine Moisture Burst Shampoo - 16 fl oz  \n",
       "4      Citre Shine Moisture Burst Shampoo - 16 fl oz  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the path to data file\n",
    "data_path = os.path.join(current_dir, 'merged_data.csv')\n",
    "print(f'data path: {data_path}')\n",
    "# load data (full dataset available at http://groups.di.unipi.it/~gulli/AG_corpus_of_news_products.html)\n",
    "dataset_path = data_path\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "df.head(n_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title: Reach Dentotape Waxed Dental Floss with Extra Wide Cleaning Surface for Large Spaces between Teeth, Unflavored, 100 Yards\n",
      "Review: best floss i've used. does not break as easily as others, and i have tight teeth.\n",
      "Rating: 5.0\n",
      "\n",
      "Title: Reach Dentotape Waxed Dental Floss with Extra Wide Cleaning Surface for Large Spaces between Teeth, Unflavored, 100 Yards\n",
      "Review: best floss i've used. does not break as easily as others, and i have tight teeth.\n",
      "Rating: 5.0\n",
      "\n",
      "Title: Citre Shine Moisture Burst Shampoo - 16 fl oz\n",
      "Review: Doesnt smell\n",
      "Rating: 2.0\n",
      "\n",
      "Title: Citre Shine Moisture Burst Shampoo - 16 fl oz\n",
      "Review: Doesnt smell\n",
      "Rating: 2.0\n",
      "\n",
      "Title: Citre Shine Moisture Burst Shampoo - 16 fl oz\n",
      "Review: My favorite powder!\n",
      "Rating: 5.0\n"
     ]
    }
   ],
   "source": [
    "# print the title, reviewText, and rating of each example\n",
    "for idx, row in df.head(n_examples).iterrows():\n",
    "    print(\"\")\n",
    "    print(f\"Title: {row['title']}\")\n",
    "    print(f\"Review: {row['reviewText']}\")\n",
    "    print(f\"Rating: {row['rating']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build cache to save embeddings\n",
    "\n",
    "+ Save our embeddings so we can re-use them later.\n",
    "+ The cache is a dictionary that maps tuples of `(text, model)` to an embedding, which is a list of floats. The cache is saved as a Python pickle file.\n",
    "+ The embedded vectors are a numerical representation of the input text's meaning, capturing both its inherent semantics and its context within the provided input. \n",
    "+ OpenAI embeddings are normalized to length 1, which means that:\n",
    "    + Cosine similarity can be computed slightly faster using just a dot product\n",
    "    + Cosine similarity and Euclidean distance will result in the identical rankings\n",
    "+ Aggregation process of embedding is not documented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish a cache of embeddings to avoid recomputing\n",
    "# cache is a dict of tuples (text, model) -> embedding, saved as a pickle file\n",
    "\n",
    "# set path to embedding cache\n",
    "# Construct the path to data file\n",
    "embedding_cache_path = os.path.join(current_dir, 'amazon_embeddings_cache.pkl')\n",
    "\n",
    "# load the cache if it exists, and save a copy to disk\n",
    "try:\n",
    "    embedding_cache = pd.read_pickle(embedding_cache_path)\n",
    "except FileNotFoundError:\n",
    "    embedding_cache = {}\n",
    "with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "    pickle.dump(embedding_cache, embedding_cache_file)\n",
    "\n",
    "# define a function to retrieve embeddings from the cache if present, and otherwise request via the API\n",
    "def embedding_from_string(\n",
    "    string: str,\n",
    "    model: str = EMBEDDING_MODEL,\n",
    "    embedding_cache=embedding_cache\n",
    ") -> list:\n",
    "    \"\"\"Return embedding of given string, using a cache to avoid recomputing.\"\"\"\n",
    "    if (string, model) not in embedding_cache.keys():\n",
    "        embedding_cache[(string, model)] = get_embedding(string, model)\n",
    "        with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "            pickle.dump(embedding_cache, embedding_cache_file)\n",
    "    return embedding_cache[(string, model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example string: Reach Dentotape Waxed Dental Floss with Extra Wide Cleaning Surface for Large Spaces between Teeth, Unflavored, 100 Yards\n",
      "\n",
      "Example embedding: [0.008420577272772789, 0.004031274002045393, 0.006344694644212723, -0.005752569064497948, 0.0015130186220631003, 0.003907340578734875, 0.001457076519727707, -0.006193220615386963, -0.007332718465477228, 0.0009880235884338617]...\n"
     ]
    }
   ],
   "source": [
    "# as an example, take the first title from the dataset\n",
    "example_string = df[\"title\"].values[0]\n",
    "print(f\"\\nExample string: {example_string}\")\n",
    "\n",
    "# print the first 10 dimensions of the embedding\n",
    "example_embedding = embedding_from_string(example_string)\n",
    "print(f\"\\nExample embedding: {example_embedding[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommend similar products based on embeddings\n",
    "\n",
    "+ Get the similarity embeddings of all the product title\n",
    "+ Calculate the distance between a source title and all other products\n",
    "+ Print out the other products closest to the source title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source string: Reach Dentotape Waxed Dental Floss with Extra Wide Cleaning Surface for Large Spaces between Teeth, Unflavored, 100 Yards\n",
      "\n",
      "        --- Recommendation #1 (nearest neighbor 1 of 5) ---\n",
      "        String: Astra Platinum Double Edge Safety Razor Blades ,100 Blades (20 x 5)\n",
      "        Distance: 0.204\n",
      "\n",
      "        --- Recommendation #2 (nearest neighbor 2 of 5) ---\n",
      "        String: Zapzyt Maximum Strength 10% Benzoyl Peroxide Acne Treatment Gel, 1 Ounce\n",
      "        Distance: 0.215\n",
      "\n",
      "        --- Recommendation #3 (nearest neighbor 3 of 5) ---\n",
      "        String: Zapzyt Maximum Strength 10% Benzoyl Peroxide Acne Treatment Gel, 1 Ounce\n",
      "        Distance: 0.215\n",
      "\n",
      "        --- Recommendation #4 (nearest neighbor 4 of 5) ---\n",
      "        String: Avalon Grapefruit and Geranium Smoothing Shampoo, 11 Ounce\n",
      "        Distance: 0.233\n",
      "\n",
      "        --- Recommendation #5 (nearest neighbor 5 of 5) ---\n",
      "        String: Avalon Grapefruit and Geranium Smoothing Shampoo, 11 Ounce\n",
      "        Distance: 0.233\n"
     ]
    }
   ],
   "source": [
    "def print_recommendations_from_strings(\n",
    "    strings: list[str],\n",
    "    index_of_source_string: int,\n",
    "    k_nearest_neighbors: int = 1,\n",
    "    model=EMBEDDING_MODEL,\n",
    ") -> list[int]:\n",
    "    \"\"\"Print out the k nearest neighbors of a given string.\"\"\"\n",
    "    # get embeddings for all strings\n",
    "    embeddings = [embedding_from_string(string, model=model) for string in strings]\n",
    "    # get the embedding of the source string\n",
    "    query_embedding = embeddings[index_of_source_string]\n",
    "    # get distances between the source embedding and other embeddings (function from embeddings_utils.py)\n",
    "    distances = distances_from_embeddings(query_embedding, embeddings, distance_metric=\"cosine\")\n",
    "    # get indices of nearest neighbors (function from embeddings_utils.py)\n",
    "    indices_of_nearest_neighbors = indices_of_nearest_neighbors_from_distances(distances)\n",
    "\n",
    "    # print out source string\n",
    "    query_string = strings[index_of_source_string]\n",
    "    print(f\"Source string: {query_string}\")\n",
    "    # print out its k nearest neighbors\n",
    "    k_counter = 0\n",
    "    for i in indices_of_nearest_neighbors:\n",
    "        # skip any strings that are identical matches to the starting string\n",
    "        if query_string == strings[i]:\n",
    "            continue\n",
    "        # stop after printing out k products\n",
    "        if k_counter >= k_nearest_neighbors:\n",
    "            break\n",
    "        k_counter += 1\n",
    "\n",
    "        # print out the similar strings and their distances\n",
    "        print(\n",
    "            f\"\"\"\n",
    "        --- Recommendation #{k_counter} (nearest neighbor {k_counter} of {k_nearest_neighbors}) ---\n",
    "        String: {strings[i]}\n",
    "        Distance: {distances[i]:0.3f}\"\"\"\n",
    "        )\n",
    "\n",
    "    return indices_of_nearest_neighbors\n",
    "\n",
    "product_titles = df[\"title\"].tolist()\n",
    "\n",
    "tony_blair_products = print_recommendations_from_strings(\n",
    "    strings=product_titles,  # let's base similarity off of the product title\n",
    "    index_of_source_string=0,  # let's look at products similar to the first one about\n",
    "    k_nearest_neighbors=5,  # let's look at the 5 most similar products\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate RMSE and MAE manually\n",
    "def calculate_rmse_and_mae(actual_ratings, predicted_ratings):\n",
    "    differences = [actual - predicted for actual, predicted in zip(actual_ratings, predicted_ratings)]\n",
    "    \n",
    "    # RMSE\n",
    "    squared_differences = [diff ** 2 for diff in differences]\n",
    "    mean_squared_difference = sum(squared_differences) / len(squared_differences)\n",
    "    rmse = mean_squared_difference ** 0.5\n",
    "\n",
    "    # MAE\n",
    "    absolute_differences = [abs(diff) for diff in differences]\n",
    "    mae = sum(absolute_differences) / len(absolute_differences)\n",
    "\n",
    "    return rmse, mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression using embedding\n",
    "\n",
    "+ Obtain embeddings for each unique user ID.\n",
    "+ For each data point, concatenate the title embedding with the user embedding to form a combined feature vector.\n",
    "+ Split the dataset into training and test sets.\n",
    "+ Train the model on the combined embeddings and predict the test set.\n",
    "+ Evaluate using RMSE and MAE metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI embedding performance: rmse=1.59, mae=1.13\n",
      "CPU times: user 370 ms, sys: 13.5 ms, total: 383 ms\n",
      "Wall time: 3.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def get_embeddings(texts: list[str], model=\"text-embedding-ada-002\") -> list[list[float]]:\n",
    "    return [item[\"embedding\"] for item in openai.Embedding.create(input=texts, model=model)[\"data\"]]\n",
    "\n",
    "# Get embeddings for titles in batches\n",
    "batch_size = 100\n",
    "title_embeddings = []\n",
    "\n",
    "for i in range(0, len(df['title']), batch_size):\n",
    "    batch_texts = df['title'].iloc[i:i+batch_size].tolist()\n",
    "    title_embeddings.extend(get_embeddings(batch_texts))\n",
    "\n",
    "# Get embeddings for unique users\n",
    "unique_users = df['reviewerID'].unique().tolist()\n",
    "user_embeddings_dict = {}\n",
    "user_embeddings = get_embeddings(unique_users)\n",
    "for user, embedding in zip(unique_users, user_embeddings):\n",
    "    user_embeddings_dict[user] = embedding\n",
    "\n",
    "# Create combined embeddings: title_embedding + user_embedding\n",
    "combined_embeddings = []\n",
    "for idx, row in df.iterrows():\n",
    "    combined_embedding = title_embeddings[idx] + user_embeddings_dict[row['reviewerID']]\n",
    "    combined_embeddings.append(combined_embedding)\n",
    "\n",
    "X_openai = np.array(combined_embeddings)\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train_openai, X_test_openai, y_train, y_test = train_test_split(X_openai, df['rating'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train RandomForest on OpenAI embeddings\n",
    "rfr_openai = RandomForestRegressor(n_estimators=100)\n",
    "rfr_openai.fit(X_train_openai, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "preds_openai = rfr_openai.predict(X_test_openai)\n",
    "rmse_openai = np.sqrt(mean_squared_error(y_test, preds_openai))  # Calculating RMSE\n",
    "mae_openai = mean_absolute_error(y_test, preds_openai)\n",
    "\n",
    "print(f\"OpenAI embedding performance: rmse={rmse_openai:.2f}, mae={mae_openai:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "+ https://cookbook.openai.com/examples/recommendation_using_embeddings\n",
    "+ https://github.com/openai/openai-python/blob/main/openai/embeddings_utils.py\n",
    "+ https://help.openai.com/en/products/6824809-embeddings-frequently-asked-questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
